{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b32bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mathlib__Algebra__Ring__Subring__Pointwise.lean.pkl\n",
      "12680 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_folder = '/home/mcwave/code/automath/atp/datasets/provability/mathlib4_states_w_proof/'\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "for file_name in file_names:\n",
    "    if not file_name.endswith(\"pkl\"):\n",
    "        continue\n",
    "    if not 'Algebra' in file_name:\n",
    "        continue\n",
    "    count += 1\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    print(\"Loading\", file_name)\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    fin = open(file_path, 'rb')\n",
    "    while True:\n",
    "        try:\n",
    "            pair = pickle.load(fin)\n",
    "            data.append(pair) #(pair[1][0], pair[1][2][0]))\n",
    "        except:\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d57ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('https://github.com/leanprover-community/mathlib4',\n",
       "  '27c6744e1c0e25d676be5eb252cd4b6d30c6acc7',\n",
       "  'Mathlib/Algebra/Ring/Subring/Pointwise.lean',\n",
       "  'Subring.pointwise_smul_def'),\n",
       " ('case h\\nM : Type u_1\\nR : Type u_2\\ninstâœÂ² : Monoid M\\ninstâœÂ¹ : Ring R\\ninstâœ : MulSemiringAction M R\\na : M\\nS : Subring R\\nnvar0 : R\\nâŠ¢ nvar0 âˆˆ SMul.smul a S â†” nvar0 âˆˆ map (MulSemiringAction.toRingHom M R a) S',\n",
       "  2,\n",
       "  ['simp_rw [mem_map]', 'exact']))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782ea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE_PP:\n",
      "case h\n",
      "M : Type u_1\n",
      "R : Type u_2\n",
      "instâœÂ² : Monoid M\n",
      "instâœÂ¹ : Ring R\n",
      "instâœ : MulSemiringAction M R\n",
      "a : M\n",
      "S : Subring R\n",
      "nvar0 : R\n",
      "âŠ¢ nvar0 âˆˆ map (MulSemiringAction.toRingHom M R a) S â†” nvar0 âˆˆ a â€¢ S\n",
      "TACTICS:\n",
      "symm\n",
      "simp [eq_comm (a := a)]\n",
      "cases S\n",
      "rw [smul_neg]\n",
      "rw [â† eq_fâ‚€']\n",
      "STATE_PP:\n",
      "theorem Subring.pointwise_smul_def (M: Type u_1) (R: Type u_2) (instâœÂ²: Monoid M) (instâœÂ¹: Ring R) (instâœ: MulSemiringAction M R) (a: M) (S: Subring R) (nvar0: R) : nvar0 âˆˆ map (MulSemiringAction.toRingHom M R a) S â†” nvar0 âˆˆ a â€¢ S :=\n",
      "\n",
      "HYPOTHESES:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from utils.lean_math_utils import *\n",
    "from utils.lean_theorem_utils import *\n",
    "\n",
    "def count_lines(string):\n",
    "    # Split the string into lines\n",
    "    lines = string.splitlines()\n",
    "    # Count the number of lines\n",
    "    return len(lines)\n",
    "\n",
    "def extract_first_case(state_pp):\n",
    "    state_pp = state_pp.strip()\n",
    "    if not state_pp.startswith('case'):\n",
    "        return state_pp\n",
    "    lines = state_pp.split('\\n')\n",
    "    first_case = []\n",
    "    for line in lines[1:]:\n",
    "        if line.strip().startswith('case'):\n",
    "            break\n",
    "        if line.strip() != '':\n",
    "            first_case.append(line)\n",
    "    return '\\n'.join(first_case)\n",
    "\n",
    "\n",
    "# Params:\n",
    "#   hyp: tuple(name, type)\n",
    "#   tactics: list(tactic)\n",
    "def is_hypothesis_useful(hyp, tactics):\n",
    "    for tactic in tactics:\n",
    "        tokens = tokenize_lean_tactic(tactic)\n",
    "        if hyp[0] in tokens:\n",
    "            idx = tokens.index(hyp[0])\n",
    "            if idx > 0:\n",
    "                if hyp[0].startswith('h'):\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'exact':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'at':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == '[':\n",
    "                    return True\n",
    "                if idx < len(tokens) - 1 and tokens[idx + 1] == ']':\n",
    "                    return True\n",
    "                for operator in TargetNode.operators:\n",
    "                    if operator in hyp[1]:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def create_hypothesis_predict_data(raw_state_pp, tactics, theorem_name):\n",
    "    is_case = raw_state_pp.strip().startswith('case')\n",
    "    state_pp = extract_first_case(raw_state_pp)\n",
    "    if is_case and count_lines(state_pp) < count_lines(raw_state_pp) - 2:\n",
    "        tactics = tactics[0:1]\n",
    "    #\n",
    "    premise = Premise()\n",
    "    premise.theorem_name = theorem_name\n",
    "    premise.parse_state(state_pp)\n",
    "    #\n",
    "    useful_hypotheses, useless_hypotheses = [], OrderedDict()\n",
    "    for hyp in premise.hypotheses.items():\n",
    "        useful = is_hypothesis_useful(hyp, tactics)\n",
    "        if useful:\n",
    "            #print(\"YES:\", hyp)\n",
    "            useful_hypotheses.append(hyp)\n",
    "        else:\n",
    "            #print(\"NO :\", hyp)\n",
    "            useless_hypotheses[hyp[0]] = hyp[1]\n",
    "    premise.hypotheses = useless_hypotheses\n",
    "    return premise, useful_hypotheses\n",
    "\n",
    "idx = 120\n",
    "\n",
    "state_pp = data[idx][1][0]\n",
    "tactics = data[idx][1][2]\n",
    "theorem_name = data[idx][0][3]\n",
    "\n",
    "print(\"STATE_PP:\\n\" + state_pp)\n",
    "\n",
    "print(\"TACTICS:\\n\" + \"\\n\".join(tactics))\n",
    "\n",
    "premise, useful_hypotheses = create_hypothesis_predict_data(state_pp, tactics, theorem_name)\n",
    "\n",
    "print(\"STATE_PP:\\n\" + premise.to_theorem_code())\n",
    "print(\"\\nHYPOTHESES:\\n\", useful_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fadb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281950\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "MIN_LENGTH = 4\n",
    "\n",
    "state_pps = []\n",
    "target_hyps = []\n",
    "seen_hashes = set()\n",
    "fin = open('/home/mcwave/code/axiomatization/datasets/mathlib4_algebra_states_w_proof_hyp_pred.pkl', 'rb')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        premise, hypotheses = pickle.load(fin)\n",
    "        state_pp = premise.to_theorem_code()\n",
    "        target_hyp = str([x[1] for x in hypotheses])\n",
    "        hash_value = hash(state_pp + '|' + target_hyp)\n",
    "        if hash_value in seen_hashes:\n",
    "            continue\n",
    "        else:\n",
    "            seen_hashes.add(hash_value)\n",
    "        #data.append((state_pp, target_hyp))\n",
    "        if len(state_pp) < 4 or len(target_hyp) < 4:\n",
    "            continue\n",
    "        state_pps.append(state_pp)\n",
    "        target_hyps.append(target_hyp)\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "fin.close()\n",
    "\n",
    "print(len(state_pps))\n",
    "\n",
    "# ds = Dataset.from_dict({'state_pp': state_pps, 'target_hyp':target_hyps})\n",
    "# tmp = ds.train_test_split(test_size=0.01)\n",
    "# raw_train_dataset = tmp['train']\n",
    "# raw_test_dataset = tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb771e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed082137c742f19cf8644afadcf1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/281950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Assuming you have two lists: instructions and responses\n",
    "instructions = state_pps #[\"Instruction 1\", \"Instruction 2\"]\n",
    "responses = target_hyps #[\"Response 1\", \"Response 2\"]\n",
    "#instructions = [\"Instruction 1\", \"Instruction 2\"]\n",
    "#responses = [\"Response 1\", \"Response 2\"]\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"datasets/text_for_tokenization/mathlib4_20240617_bpe_tokenizer\")\n",
    "# Define the tokens\n",
    "sep_token = \"<sep>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "# Set the sep_token and pad_token\n",
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.pad_token = pad_token\n",
    "#tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} <sep> {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "    \n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == tokenizer.sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "        else:\n",
    "            sep_token_idx = sep_token_idx.item()\n",
    "\n",
    "        attention_mask[0:end_idx] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx+1] = -100\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# Create the Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'instruction': instructions,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Apply the tokenization and preparation function\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_data,\n",
    "    batched=True\n",
    "    #remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae1f6b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[100000]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920ae58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenized_dataset.train_test_split(test_size=0.02)\n",
    "tokenized_train = tmp['train']\n",
    "tokenized_test = tmp['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c0111bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(15000, 1024)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"xhyi/PT_GPTNEO350_ATG\"  # You can change this to a larger GPTNeo model if needed\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/atp/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6771' max='34540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6771/34540 1:15:45 < 5:10:45, 1.49 it/s, Epoch 0.78/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.231519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/predict-hyp-gptneo-350m\",\n",
    "    evaluation_strategy=\"steps\", #\"epochs\"\n",
    "    learning_rate=2e-5,  # PAY ATTENTION TO LEARNING RATE!\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    save_steps=5000,\n",
    "    eval_steps=5000,\n",
    "    logging_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "#cp_path = 'gpt-neo-350m-202310/checkpoint-525000'\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ccebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT-Neo 350M\n",
    "batch_size 32\n",
    "\n",
    "Step\tTraining Loss\tValidation Loss\n",
    "5000\t0.399900\t0.231519"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atp",
   "language": "python",
   "name": "atp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
