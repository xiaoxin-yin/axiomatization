{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b32bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mathlib__Algebra__Ring__Subring__Pointwise.lean.pkl\n",
      "12680 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_folder = '/home/mcwave/code/automath/atp/datasets/provability/mathlib4_states_w_proof/'\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "for file_name in file_names:\n",
    "    if not file_name.endswith(\"pkl\"):\n",
    "        continue\n",
    "    if not 'Algebra' in file_name:\n",
    "        continue\n",
    "    count += 1\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    print(\"Loading\", file_name)\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    fin = open(file_path, 'rb')\n",
    "    while True:\n",
    "        try:\n",
    "            pair = pickle.load(fin)\n",
    "            data.append(pair) #(pair[1][0], pair[1][2][0]))\n",
    "        except:\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782ea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE_PP:\n",
      "case h\n",
      "M : Type u_1\n",
      "R : Type u_2\n",
      "inst‚úù¬≤ : Monoid M\n",
      "inst‚úù¬π : Ring R\n",
      "inst‚úù : MulSemiringAction M R\n",
      "a : M\n",
      "S : Subring R\n",
      "nvar0 : R\n",
      "‚ä¢ nvar0 ‚àà map (MulSemiringAction.toRingHom M R a) S ‚Üî nvar0 ‚àà a ‚Ä¢ S\n",
      "TACTICS:\n",
      "symm\n",
      "simp [eq_comm (a := a)]\n",
      "cases S\n",
      "rw [smul_neg]\n",
      "rw [‚Üê eq_f‚ÇÄ']\n",
      "STATE_PP:\n",
      "theorem Subring.pointwise_smul_def (M: Type u_1) (R: Type u_2) (inst‚úù¬≤: Monoid M) (inst‚úù¬π: Ring R) (inst‚úù: MulSemiringAction M R) (a: M) (S: Subring R) (nvar0: R) : nvar0 ‚àà map (MulSemiringAction.toRingHom M R a) S ‚Üî nvar0 ‚àà a ‚Ä¢ S :=\n",
      "\n",
      "HYPOTHESES:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from utils.lean_math_utils import *\n",
    "from utils.lean_theorem_utils import *\n",
    "\n",
    "def count_lines(string):\n",
    "    # Split the string into lines\n",
    "    lines = string.splitlines()\n",
    "    # Count the number of lines\n",
    "    return len(lines)\n",
    "\n",
    "def extract_first_case(state_pp):\n",
    "    state_pp = state_pp.strip()\n",
    "    if not state_pp.startswith('case'):\n",
    "        return state_pp\n",
    "    lines = state_pp.split('\\n')\n",
    "    first_case = []\n",
    "    for line in lines[1:]:\n",
    "        if line.strip().startswith('case'):\n",
    "            break\n",
    "        if line.strip() != '':\n",
    "            first_case.append(line)\n",
    "    return '\\n'.join(first_case)\n",
    "\n",
    "\n",
    "# Params:\n",
    "#   hyp: tuple(name, type)\n",
    "#   tactics: list(tactic)\n",
    "def is_hypothesis_useful(hyp, tactics):\n",
    "    for tactic in tactics:\n",
    "        tokens = tokenize_lean_tactic(tactic)\n",
    "        if hyp[0] in tokens:\n",
    "            idx = tokens.index(hyp[0])\n",
    "            if idx > 0:\n",
    "                if hyp[0].startswith('h'):\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'exact':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'at':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == '[':\n",
    "                    return True\n",
    "                if idx < len(tokens) - 1 and tokens[idx + 1] == ']':\n",
    "                    return True\n",
    "                for operator in TargetNode.operators:\n",
    "                    if operator in hyp[1]:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def create_hypothesis_predict_data(raw_state_pp, tactics, theorem_name):\n",
    "    is_case = raw_state_pp.strip().startswith('case')\n",
    "    state_pp = extract_first_case(raw_state_pp)\n",
    "    if is_case and count_lines(state_pp) < count_lines(raw_state_pp) - 2:\n",
    "        tactics = tactics[0:1]\n",
    "    #\n",
    "    premise = Premise()\n",
    "    premise.theorem_name = theorem_name\n",
    "    premise.parse_state(state_pp)\n",
    "    #\n",
    "    useful_hypotheses, useless_hypotheses = [], OrderedDict()\n",
    "    for hyp in premise.hypotheses.items():\n",
    "        useful = is_hypothesis_useful(hyp, tactics)\n",
    "        if useful:\n",
    "            #print(\"YES:\", hyp)\n",
    "            useful_hypotheses.append(hyp)\n",
    "        else:\n",
    "            #print(\"NO :\", hyp)\n",
    "            useless_hypotheses[hyp[0]] = hyp[1]\n",
    "    premise.hypotheses = useless_hypotheses\n",
    "    return premise, useful_hypotheses\n",
    "\n",
    "idx = 120\n",
    "\n",
    "state_pp = data[idx][1][0]\n",
    "tactics = data[idx][1][2]\n",
    "theorem_name = data[idx][0][3]\n",
    "\n",
    "print(\"STATE_PP:\\n\" + state_pp)\n",
    "\n",
    "print(\"TACTICS:\\n\" + \"\\n\".join(tactics))\n",
    "\n",
    "premise, useful_hypotheses = create_hypothesis_predict_data(state_pp, tactics, theorem_name)\n",
    "\n",
    "print(\"STATE_PP:\\n\" + premise.to_theorem_code())\n",
    "print(\"\\nHYPOTHESES:\\n\", useful_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fadb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3396064\n",
      "Test: 24237\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "MIN_LENGTH = 4\n",
    "\n",
    "TEST_MOD = 130\n",
    "\n",
    "train_state_pps = []\n",
    "test_state_pps = []\n",
    "train_target_hyps = []\n",
    "test_target_hyps = []\n",
    "seen_hashes = set()\n",
    "fin = open('/home/mcwave/code/axiomatization/datasets/mathlib4_all_states_w_proof_hyp_pred.pkl', 'rb')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        premise, hypotheses = pickle.load(fin)\n",
    "        state_pp = premise.to_theorem_code()\n",
    "        target_hyp = str([x[1] for x in hypotheses])\n",
    "        hash_value = hash(state_pp + '|' + target_hyp)\n",
    "        if hash_value in seen_hashes:\n",
    "            continue\n",
    "        else:\n",
    "            seen_hashes.add(hash_value)\n",
    "        #data.append((state_pp, target_hyp))\n",
    "        if len(state_pp) < 4 or len(target_hyp) < 4:\n",
    "            continue\n",
    "        if hash(premise.theorem_name) % TEST_MOD == 0:\n",
    "            test_state_pps.append(state_pp)\n",
    "            test_target_hyps.append(target_hyp)\n",
    "        else:\n",
    "            train_state_pps.append(state_pp)\n",
    "            train_target_hyps.append(target_hyp)\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "fin.close()\n",
    "\n",
    "print(\"Train:\", len(train_state_pps))\n",
    "print(\"Test:\", len(test_state_pps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "model_name = \"morph-labs/morph-prover-v0-7b\" #\"internlm/internlm2-math-7b\" #\"ScalableMath/Lean-STaR-plus\"  # 'Saisam/gpt-neo-math-small' #\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Define the separator token\n",
    "sep_token = \"<sep>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "# Check if the separator token already exists in the vocabulary\n",
    "if sep_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([sep_token])\n",
    "if pad_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Set the separator token\n",
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'sep_token': sep_token,\n",
    "    'pad_token': pad_token\n",
    "})\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"datasets/text_for_tokenization/mathlib4_20240617_bpe_tokenizer\")\n",
    "# # Define the tokens\n",
    "# sep_token = \"<sep>\"\n",
    "# pad_token = \"<pad>\"\n",
    "\n",
    "# # Set the sep_token and pad_token\n",
    "# tokenizer.sep_token = sep_token\n",
    "# tokenizer.pad_token = pad_token\n",
    "# #tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} <sep> {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "    \n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == tokenizer.sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "        else:\n",
    "            sep_token_idx = sep_token_idx.item()\n",
    "\n",
    "        attention_mask[0:end_idx] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx+1] = -100\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# test_dataset = Dataset.from_dict({\n",
    "#     'instruction': test_state_pps,\n",
    "#     'response': test_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_test = test_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     'instruction': train_state_pps,\n",
    "#     'response': train_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_train = train_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920ae58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89734dc8df74f6198ce6d17b3e04dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "#tokenized_train.save_to_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "#tokenized_test.save_to_disk('datasets/predict_hyp_tokenized_test.dataset')\n",
    "\n",
    "tokenized_train = load_from_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "tokenized_test = load_from_disk('datasets/predict_hyp_tokenized_test.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "777956fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'theorem Finset.diffs_union_right (F: Type u_1) (Œ±: Type u_2) (Œ≤: Type u_3) (inst‚úù¬≤: DecidableEq Œ±) (inst‚úù¬π: DecidableEq Œ≤) (inst‚úù: GeneralizedBooleanAlgebra Œ±) (s‚ÇÅ s‚ÇÇ t t‚ÇÅ t‚ÇÇ u v: Finset Œ±) (a b c: Œ±) (val‚úù: Multiset Œ±) (nodup‚úù: val‚úù.Nodup) (‚ä¢: = val‚úù, nodup := nodup‚úù } \\\\\\\\ (t‚ÇÅ ‚à™ t‚ÇÇ \\\\ t‚ÇÅ) =) ({: = val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÇ) :  :=',\n",
       " 'response': \"['= val‚úù, nodup := nodup‚úù } \\\\\\\\\\\\\\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\\\\\\\\\\\\\ t‚ÇÇ']\",\n",
       " 'input_ids': [1,\n",
       "  16980,\n",
       "  3727,\n",
       "  673,\n",
       "  28723,\n",
       "  28715,\n",
       "  17820,\n",
       "  28730,\n",
       "  14324,\n",
       "  28730,\n",
       "  1246,\n",
       "  325,\n",
       "  28765,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28740,\n",
       "  28731,\n",
       "  325,\n",
       "  28948,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28750,\n",
       "  28731,\n",
       "  325,\n",
       "  29152,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28770,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28941,\n",
       "  28747,\n",
       "  6712,\n",
       "  313,\n",
       "  522,\n",
       "  14564,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  30047,\n",
       "  28747,\n",
       "  6712,\n",
       "  313,\n",
       "  522,\n",
       "  14564,\n",
       "  28705,\n",
       "  29152,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  3592,\n",
       "  1332,\n",
       "  9925,\n",
       "  2707,\n",
       "  10343,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  28713,\n",
       "  31552,\n",
       "  268,\n",
       "  30202,\n",
       "  261,\n",
       "  261,\n",
       "  31552,\n",
       "  261,\n",
       "  30202,\n",
       "  332,\n",
       "  363,\n",
       "  28747,\n",
       "  3727,\n",
       "  673,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  28708,\n",
       "  287,\n",
       "  277,\n",
       "  28747,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  1052,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  9713,\n",
       "  278,\n",
       "  299,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  26177,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28723,\n",
       "  28759,\n",
       "  350,\n",
       "  715,\n",
       "  28731,\n",
       "  325,\n",
       "  229,\n",
       "  141,\n",
       "  165,\n",
       "  28747,\n",
       "  327,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  325,\n",
       "  28707,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  261,\n",
       "  30202,\n",
       "  414,\n",
       "  261,\n",
       "  31552,\n",
       "  28731,\n",
       "  327,\n",
       "  28731,\n",
       "  13734,\n",
       "  28747,\n",
       "  327,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  261,\n",
       "  30202,\n",
       "  28731,\n",
       "  714,\n",
       "  28705,\n",
       "  2137,\n",
       "  28705,\n",
       "  32000,\n",
       "  28705,\n",
       "  5936,\n",
       "  28746,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  30202,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  28705,\n",
       "  5936,\n",
       "  28746,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  30202,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68be9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db0e3d3fa37496cb02f2d355628da2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32002, 2048, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "#from huggingface_hub import login\n",
    "\n",
    "#login(token=\"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\")\n",
    "\n",
    "model_name = \"google/gemma-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/axiom/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286047' max='1132024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 286047/1132024 12:18:06 < 226:01:02, 1.04 it/s, Epoch 1.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.167491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.169526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/predict-hyp-gemma-2b\",\n",
    "    evaluation_strategy=\"steps\", #\"epochs\"\n",
    "    learning_rate=3e-7,  # PAY ATTENTION TO LEARNING RATE!\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=20000,\n",
    "    eval_steps=20000,\n",
    "    logging_steps=20000,\n",
    "    save_total_limit=3,\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "cp_path = 'datasets/predict-hyp-gemma-2b/checkpoint-240000'\n",
    "\n",
    "trainer.train(cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f642f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> theorem CategoryTheory.mop_rightUnitor (C: Type u‚ÇÅ) (inst‚úù¬π: Category.{v‚ÇÅ, u‚ÇÅ} C) (inst‚úù: MonoidalCategory C) (X: C) (‚ä¢ ¬¨¬¨¬¨¬¨(œÅ_ X).mop Œª_ { unmop: = X }) :  := <sep>  ['¬¨¬¨¬¨(œÅ_ X).mop = Œª_ { unmop := X }', '= X }']<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458582ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE 0\n",
      "inputs: theorem LinearMap.exact_map_mkQ_range (R: Type u_1) (M: Type u_2) (N: Type u_3) (P: Type u_4) (inst‚úù‚Å∂: CommRing R) (inst‚úù‚Åµ: AddCommGroup M) (inst‚úù‚Å¥: AddCommGroup N) (inst‚úù¬≥: AddCommGroup P) (inst‚úù¬≤: Module R M) (inst‚úù¬π: Module R N) (inst‚úù: Module R P) (y‚úù: N) : y‚úù ‚àà Set.range ‚áëf ‚Üî 0 = (range f).mkQ y‚úù :=\n",
      "labels:  ['M ‚Üí‚Çó[R] N']\n",
      "Generated: theorem LinearMap.exact_map_mkQ_range (R: Type u_1) (M: Type u_2) (N: Type u_3) (P: Type u_4) (inst‚úù‚Å∂: CommRing R) (inst‚úù‚Åµ: AddCommGroup M) (inst‚úù‚Å¥: AddCommGroup N) (inst‚úù¬≥: AddCommGroup P) (inst‚úù¬≤: Module R M) (inst‚úù¬π: Module R N) (inst‚úù: Module R P) (y‚úù: N) : y‚úù ‚àà Set.range ‚áëf ‚Üî 0 = (range f).mkQ y‚úù :=   ['M ‚Üí‚Çó[R] N']']'] 0 = 0 ‚Üî 0 = 0']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 1\n",
      "inputs: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst‚úù¬≤: CommRing R) (inst‚úù¬π: DistribSMul S R) (inst‚úù: IsScalarTower S R R) (a x ‚ä¢ Quotient.map' (fun => ‚Ä¢ x) ‚ãØ ((mk {: = toFinsupp‚úù }) x) = (mk { toFinsupp := toFinsupp‚úù }) (a ‚Ä¢ x)) (toFinsupp‚úù: AddMonoidAlgebra R ‚Ñï) :  :=\n",
      "labels:  ['= toFinsupp‚úù }) x) = (mk { toFinsupp := toFinsupp‚úù }) (a ‚Ä¢ x)']\n",
      "Generated: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst‚úù¬≤: CommRing R) (inst‚úù¬π: DistribSMul S R) (inst‚úù: IsScalarTower S R R) (a x ‚ä¢ Quotient.map' (fun => ‚Ä¢ x) ‚ãØ ((mk {: = toFinsupp‚úù }) x) = (mk { toFinsupp := toFinsupp‚úù }) (a ‚Ä¢ x)) (toFinsupp‚úù: AddMonoidAlgebra R ‚Ñï) :  :=   ['= toFinsupp‚úù }) x) = (mk { toFinsupp := toFinsupp‚úù }) (a ‚Ä¢ x)']']']']']']']']']']']']']\n",
      "\n",
      "CASE 2\n",
      "inputs: theorem Finset.diffs_union_right (F: Type u_1) (Œ±: Type u_2) (Œ≤: Type u_3) (inst‚úù¬≤: DecidableEq Œ±) (inst‚úù¬π: DecidableEq Œ≤) (inst‚úù: GeneralizedBooleanAlgebra Œ±) (s‚ÇÅ s‚ÇÇ t t‚ÇÅ t‚ÇÇ u v: Finset Œ±) (a b c: Œ±) (val‚úù: Multiset Œ±) (nodup‚úù: val‚úù.Nodup) (‚ä¢: = val‚úù, nodup := nodup‚úù } \\\\ (t‚ÇÅ ‚à™ t‚ÇÇ \\ t‚ÇÅ) =) ({: = val‚úù, nodup := nodup‚úù } \\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\ t‚ÇÇ) :  :=\n",
      "labels:  ['= val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÇ']\n",
      "Generated: theorem Finset.diffs_union_right (F: Type u_1) (Œ±: Type u_2) (Œ≤: Type u_3) (inst‚úù¬≤: DecidableEq Œ±) (inst‚úù¬π: DecidableEq Œ≤) (inst‚úù: GeneralizedBooleanAlgebra Œ±) (s‚ÇÅ s‚ÇÇ t t‚ÇÅ t‚ÇÇ u v: Finset Œ±) (a b c: Œ±) (val‚úù: Multiset Œ±) (nodup‚úù: val‚úù.Nodup) (‚ä¢: = val‚úù, nodup := nodup‚úù } \\\\ (t‚ÇÅ ‚à™ t‚ÇÇ \\ t‚ÇÅ) =) ({: = val‚úù, nodup := nodup‚úù } \\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\ t‚ÇÇ) :  :=   ['= val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÅ ‚à™ { val := val‚úù, nodup := nodup‚úù } \\\\\\\\ t‚ÇÇ']']\n",
      "\n",
      "CASE 3\n",
      "inputs: theorem RootPairing.coreflection_eq_flip_reflection (Œπ: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst‚úù‚Å¥: CommRing R) (inst‚úù¬≥: AddCommGroup M) (inst‚úù¬≤: Module R M) (inst‚úù¬π: AddCommGroup N) (inst‚úù: Module R N) (i j: Œπ) (f: N) (toPerfectPairing‚úù: PerfectPairing R M N) (root‚úù: Œπ ‚Ü™ M) (coroot‚úù: Œπ ‚Ü™ N) (root_coroot_two‚úù: ‚àÄ (i : Œπ), (toPerfectPairing‚úù.toLin (root‚úù i)) (coroot‚úù i) = 2) (mapsTo_preReflection_root‚úù mapsTo_preReflection_coroot‚úù: ) ((i: Œπ), MapsTo (‚áë(preReflection (coroot‚úù i) (toPerfectPairing‚úù.toLin (root‚úù i)))) (range ‚áëcoroot‚úù) (range ‚áëcoroot‚úù)) (-(({ ({: = toPerfectPairing‚úù, root := root‚úù, coroot := coroot‚úù, root_coroot_two := root_coroot_two‚úù,) (mapsTo_preReflection_root: = mapsTo_preReflection_root‚úù,) (root_coroot_two: = root_coroot_two‚úù, mapsTo_preReflection_root := mapsTo_preReflection_root‚úù,) ({: = toPerfectPairing‚úù, root := root‚úù, coroot := coroot‚úù,) : f + :=\n",
      "labels: theorem RootPairing.coreflection_eq_flip_reflection (Œπ: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst‚úù‚Å¥: CommRing R) (inst‚úù¬≥: AddCommGroup M) (inst‚úù¬≤: Module R M) (inst‚úù¬π: AddCommGroup N) (inst‚úù: Module R N) (i j: Œπ) (f: N) (toPerfectPairing‚úù: PerfectPairing R M N) (root‚úù: Œπ ‚Ü™ M) (coroot‚úù: Œπ ‚Ü™ N) (root_coroot_two‚úù: ‚àÄ (i : Œπ), (toPerfectPairing‚úù.toLin (root‚úù i)) (coroot‚úù i) = 2) (mapsTo_preReflection_root‚úù mapsTo_preReflection_coroot‚úù: ) ((i: Œπ), MapsTo (‚áë(preReflection (coroot‚úù i) (toPerfectPairing‚úù.toLin (root‚úù i)))) (range ‚áëcoroot‚úù) (range \n",
      "Generated: theorem RootPairing.coreflection_eq_flip_reflection (Œπ: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst‚úù‚Å¥: CommRing R) (inst‚úù¬≥: AddCommGroup M) (inst‚úù¬≤: Module R M) (inst‚úù¬π: AddCommGroup N) (inst‚úù: Module R N) (i j: Œπ) (f: N) (toPerfectPairing‚úù: PerfectPairing R M N) (root‚úù: Œπ ‚Ü™ M) (coroot‚úù: Œπ ‚Ü™ N) (root_coroot_two‚úù: ‚àÄ (i : Œπ), (toPerfectPairing‚úù.toLin (root‚úù i)) (coroot‚úù i) = 2) (mapsTo_preReflection_root‚úù mapsTo_preReflection_coroot‚úù: ) ((i: Œπ), MapsTo (‚áë(preReflection (coroot‚úù i) (toPerfectPairing‚úù.toLin (root‚úù i)))) (range ‚áëcoroot‚úù) (range ‚áëcoroot‚úù)) (-(({ ({: = toPerfectPairing‚úù, root := root‚úù, coroot := coroot‚úù, root_coroot_two := root_coroot_two‚úù,) (mapsTo_preReflection_root: = mapsTo_preReflection_root‚úù,) (root_coroot_two: = root_coroot_two‚úù, mapsTo_preReflection_root := mapsTo_preReflection_root‚úù,) ({: = toPerfectPairing‚úù, root := root‚úù, coroot := coroot‚úù,) : f + :=   ['Œπ), MapsTo (‚áë(preReflection (coroot‚úù i) (toPerfectPairing‚úù.toLin (root‚úù i)))) (range ‚áëcor\n",
      "\n",
      "CASE 4\n",
      "inputs: theorem Finsupp.prod_ne_zero_iff (Œ±: Type u_1) (Œπ: Type u_2) (Œ≥: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst‚úù‚Å∂: AddCommMonoid A) (inst‚úù‚Åµ: AddCommMonoid B) (inst‚úù‚Å¥: AddCommMonoid C) (t: Œπ ‚Üí A ‚Üí C) (h0: ‚àÄ (i : Œπ), t i 0 = 0) (h1: ‚àÄ (i : Œπ) (x y : A), t i (x + y) = t i x + t i y) (s: Finset Œ±) (f‚úù: Œ± ‚Üí Œπ ‚Üí‚ÇÄ A) (i: Œπ) (g‚úù: Œπ ‚Üí‚ÇÄ A) (k: Œπ ‚Üí A ‚Üí Œ≥ ‚Üí B) (x: Œ≥) (Œ≤: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S: Type u_15) (inst‚úù¬≥: Zero Œ±) (inst‚úù¬≤: CommMonoidWithZero Œ≤) (inst‚úù¬π: Nontrivial Œ≤) (inst‚úù: NoZeroDivisors Œ≤) (f: Œπ ‚Üí‚ÇÄ Œ±) (a: Œ±) (g: Œπ ‚Üí Œ± ‚Üí Œ≤) (‚ä¢ (‚àÄ (i: Œπ), f i ‚â† 0 ‚Üí g i (f i) ‚â† ?m.111048 * 0) ‚Üî f.prod g ‚â† ?m.111048 * 0) : Œ≤ :=\n",
      "labels: theorem Finsupp.prod_ne_zero_iff (Œ±: Type u_1) (Œπ: Type u_2) (Œ≥: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst‚úù‚Å∂: AddCommMonoid A) (inst‚úù‚Åµ: AddCommMonoid B) (inst‚úù‚Å¥: AddCommMonoid C) (t: Œπ ‚Üí A ‚Üí C) (h0: ‚àÄ (i : Œπ), t i 0 = 0) (h1: ‚àÄ (i : Œπ) (x y : A), t i (x + y) = t i x + t i y) (s: Finset Œ±) (f‚úù: Œ± ‚Üí Œπ ‚Üí‚ÇÄ A) (i: Œπ) (g‚úù: Œπ ‚Üí‚ÇÄ A) (k: Œπ ‚Üí A ‚Üí Œ≥ ‚Üí B) (x: Œ≥) (Œ≤: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S\n",
      "Generated: theorem Finsupp.prod_ne_zero_iff (Œ±: Type u_1) (Œπ: Type u_2) (Œ≥: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst‚úù‚Å∂: AddCommMonoid A) (inst‚úù‚Åµ: AddCommMonoid B) (inst‚úù‚Å¥: AddCommMonoid C) (t: Œπ ‚Üí A ‚Üí C) (h0: ‚àÄ (i : Œπ), t i 0 = 0) (h1: ‚àÄ (i : Œπ) (x y : A), t i (x + y) = t i x + t i y) (s: Finset Œ±) (f‚úù: Œ± ‚Üí Œπ ‚Üí‚ÇÄ A) (i: Œπ) (g‚úù: Œπ ‚Üí‚ÇÄ A) (k: Œπ ‚Üí A ‚Üí Œ≥ ‚Üí B) (x: Œ≥) (Œ≤: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S: Type u_15) (inst‚úù¬≥: Zero Œ±) (inst‚úù¬≤: CommMonoidWithZero Œ≤) (inst‚úù¬π: Nontrivial Œ≤) (inst‚úù: NoZeroDivisors Œ≤) (f: Œπ ‚Üí‚ÇÄ Œ±) (a: Œ±) (g: Œπ ‚Üí Œ± ‚Üí Œ≤) (‚ä¢ (‚àÄ (i: Œπ), f i ‚â† 0 ‚Üí g i (f i) ‚â† ?m.111048 * 0) ‚Üî f.prod g ‚â† ?m.111048 * 0) : Œ≤ :=   ['0 = 0']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 5\n",
      "inputs: theorem unitInterval.volume_def (nvar0: Set ‚ÜëI) (nvar1: ‚Ñù) : ‚àÉ b, (Measure.comap Subtype.val volume) nvar0 = ‚Üëb ‚àß b ‚â§ ‚ü®nvar1, nvar2‚ü© :=\n",
      "labels:  ['0 ‚â§ nvar1', 'volume nvar0 = ‚Üë‚ü®nvar1, nvar2‚ü©']\n",
      "Generated: theorem unitInterval.volume_def (nvar0: Set ‚ÜëI) (nvar1: ‚Ñù) : ‚àÉ b, (Measure.comap Subtype.val volume) nvar0 = ‚Üëb ‚àß b ‚â§ ‚ü®nvar1, nvar2‚ü© :=   ['0 ‚â§ nvar1', 'volume nvar0 = ‚Üë‚ü®nvar1, nvar2‚ü©'] '‚Üë‚ü®nvar1, nvar2‚ü© = ‚Üë‚ü®nvar1, nvar\n",
      "\n",
      "CASE 6\n",
      "inputs: theorem interior_sInter_subset (X: Type u) (Y: Type v) (Œπ: Sort w) (Œ±: Type u_1) (Œ≤: Type u_2) (x nvar0: X) (s s‚ÇÅ s‚ÇÇ t: Set X) (p p‚ÇÅ p‚ÇÇ: X ‚Üí Prop) (inst‚úù: TopologicalSpace X) (S: Set (Set X)) : nvar0 ‚àà ‚ãÇ s ‚àà S, ‚ãÉ‚ÇÄ {t | IsOpen t ‚àß t ‚äÜ s} :=\n",
      "labels:  ['nvar0 ‚àà ‚ãÉ‚ÇÄ {t | IsOpen t ‚àß t ‚äÜ ‚ãÇ‚ÇÄ S}']\n",
      "Generated: theorem interior_sInter_subset (X: Type u) (Y: Type v) (Œπ: Sort w) (Œ±: Type u_1) (Œ≤: Type u_2) (x nvar0: X) (s s‚ÇÅ s‚ÇÇ t: Set X) (p p‚ÇÅ p‚ÇÇ: X ‚Üí Prop) (inst‚úù: TopologicalSpace X) (S: Set (Set X)) : nvar0 ‚àà ‚ãÇ s ‚àà S, ‚ãÉ‚ÇÄ {t | IsOpen t ‚àß t ‚äÜ s} :=   ['nvar0 ‚àà ‚ãÉ‚ÇÄ {t | IsOpen t ‚àß t ‚äÜ ‚ãÇ‚ÇÄ S}']']']']']']']']']']\n",
      "\n",
      "CASE 7\n",
      "inputs: theorem RootPairing.coreflection_eq_flip_reflection (Œπ: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst: CommRing R) (inst_1: AddCommGroup M) (inst_3: AddCommGroup N) (inst_4: Module R N) (i j: Œπ) (f: N) (root: Œπ ‚Ü™ M) (coroot: Œπ ‚Ü™ N) (root_coroot_two: ‚àÄ (i : Œπ), (toPerfectPairing.toLin (root i)) (coroot i) = 2) (mapsTo_preReflection_root: = mapsTo_preReflection_root,) ((i: Œπ), MapsTo (‚áë(preReflection (coroot i) (toPerfectPairing.toLin (root i)))) (range ‚áëcoroot) (range ‚áëcoroot)) (‚ä¢ ({ {: = toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,) :  :=\n",
      "labels:  ['Module R M', '= toPerfectPairing, root := root, coroot := coroot,\n",
      "Generated: theorem RootPairing.coreflection_eq_flip_reflection (Œπ: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst: CommRing R) (inst_1: AddCommGroup M) (inst_3: AddCommGroup N) (inst_4: Module R N) (i j: Œπ) (f: N) (root: Œπ ‚Ü™ M) (coroot: Œπ ‚Ü™ N) (root_coroot_two: ‚àÄ (i : Œπ), (toPerfectPairing.toLin (root i)) (coroot i) = 2) (mapsTo_preReflection_root: = mapsTo_preReflection_root,) ((i: Œπ), MapsTo (‚áë(preReflection (coroot i) (toPerfectPairing.toLin (root i)))) (range ‚áëcoroot) (range ‚áëcoroot)) (‚ä¢ ({ {: = toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,) :  :=   ['Module R M', '= toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,', 'Œπ), MapsTo (ÔøΩÔøΩ\n",
      "\n",
      "CASE 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst‚úù¬≤: CommRing R) (f: R[X]) (inst‚úù¬π: DistribSMul S R) (inst‚úù: IsScalarTower S R R) (a ‚ä¢ Quotient.map' (fun => x) ‚ãØ ((mk f) = (mk (a { toFinsupp: = x.1 })) :  :=\n",
      "labels:  ['= x.1 })', '= x.1 })']\n",
      "Generated: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst‚úù¬≤: CommRing R) (f: R[X]) (inst‚úù¬π: DistribSMul S R) (inst‚úù: IsScalarTower S R R) (a ‚ä¢ Quotient.map' (fun => x) ‚ãØ ((mk f) = (mk (a { toFinsupp: = x.1 })) :  :=   ['= x.1 })']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 9\n",
      "inputs: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R‚ÇÅ: Type u_2) (R‚ÇÇ: Type u_3) (R‚ÇÉ: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M‚ÇÅ: Type u_9) (M‚ÇÇ: Type u_10) (M‚ÇÉ: Type u_11) (N‚ÇÅ: Type u_12) (N‚ÇÇ: Type u_13) (N‚ÇÉ: Type u_14) (N‚ÇÑ: Type u_15) (Œπ: Type u_16) (M‚ÇÑ: Type u_17) (inst‚úù¬π‚Å∑: Semiring R) (inst‚úù¬π‚Å∂: Semiring S) (inst‚úù¬π‚Åµ: Semiring R‚ÇÅ) (inst‚úù¬π‚Å¥: Semiring R‚ÇÇ) (inst‚úù¬π¬≥: Semiring R‚ÇÉ) (inst‚úù¬π¬≤: AddCommMonoid M) (inst‚úù¬π¬π: AddCommMonoid M‚ÇÅ) (inst‚úù¬π‚Å∞: AddCommMonoid M‚ÇÇ) (inst‚úù‚Åπ: AddCommMonoid M‚ÇÉ) (inst‚úù‚Å∏: AddCommMonoid M‚ÇÑ) (inst‚úù‚Å∑: AddCommMonoid N‚ÇÅ) (inst‚úù‚Å∂: AddCommMonoid N‚ÇÇ) (module_M: Module R M) (module_S_M‚ÇÇ: Module S M‚ÇÇ) (œÉ: R ‚Üí+* S) (œÉ': S ‚Üí+* R) (re‚ÇÅ: RingHomInvPair œÉ œÉ') (re‚ÇÇ: RingHomInvPair œÉ' œÉ) (e e': M ‚âÉ‚Çõ‚Çó[œÉ] M‚ÇÇ) (module_M‚ÇÅ: Module R‚ÇÅ M‚ÇÅ) (module_M‚ÇÇ: Module R‚ÇÇ M‚ÇÇ) (module_M‚ÇÉ: Module R‚ÇÉ M‚ÇÉ) (module_N‚ÇÅ: Module R‚ÇÅ N‚ÇÅ) (module_N‚ÇÇ: Module R‚ÇÅ N‚ÇÇ) (œÉ‚ÇÅ‚ÇÇ: R‚ÇÅ ‚Üí+* R‚ÇÇ) (œÉ‚ÇÇ‚ÇÉ: R‚ÇÇ ‚Üí+* R‚ÇÉ) (œÉ‚ÇÅ‚ÇÉ: R‚ÇÅ ‚Üí+* R‚ÇÉ) (œÉ‚ÇÇ‚ÇÅ: R‚ÇÇ ‚Üí+* R‚ÇÅ) (œÉ‚ÇÉ‚ÇÇ: R‚ÇÉ ‚Üí+* R‚ÇÇ) (œÉ‚ÇÉ‚ÇÅ: R‚ÇÉ ‚Üí+* R‚ÇÅ) (inst‚úù‚Åµ: RingHomCompTriple œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÇ‚ÇÉ œÉ‚ÇÅ‚ÇÉ) (inst‚úù‚Å¥: RingHomCompTriple œÉ‚ÇÉ‚ÇÇ œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÉ‚ÇÅ) (re‚ÇÅ‚ÇÇ: RingHomInvPair œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÇ‚ÇÅ) (re‚ÇÇ‚ÇÉ: RingHomInvPair œÉ‚ÇÇ‚ÇÉ œÉ‚ÇÉ‚ÇÇ) (inst‚úù¬≥: RingHomInvPair œÉ‚ÇÅ‚ÇÉ œÉ‚ÇÉ‚ÇÅ) (re‚ÇÇ‚ÇÅ: RingHomInvPair œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÅ‚ÇÇ) (re‚ÇÉ‚ÇÇ: RingHomInvPair œÉ‚ÇÉ‚ÇÇ œÉ‚ÇÇ‚ÇÉ) (inst‚úù¬≤: RingHomInvPair œÉ‚ÇÉ‚ÇÅ œÉ‚ÇÅ‚ÇÉ) (e‚ÇÇ‚ÇÉ: M‚ÇÇ ‚âÉ‚Çõ‚Çó[œÉ‚ÇÇ‚ÇÉ] M‚ÇÉ) (inst‚úù¬π: RingHomCompTriple œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÅ‚ÇÉ œÉ‚ÇÇ‚ÇÉ) (inst‚úù: RingHomCompTriple œÉ‚ÇÉ‚ÇÅ œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÉ‚ÇÇ) (f: M‚ÇÇ ‚Üí‚Çõ‚Çó[œÉ‚ÇÇ‚ÇÉ] M‚ÇÉ) : f.comp ‚Üëe‚ÇÅ‚ÇÇ.symm.symm = g ‚Üî g = f.comp ‚Üëe‚ÇÅ‚ÇÇ :=\n",
      "labels: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R‚ÇÅ: Type u_2) (R‚ÇÇ: Type u_3) (R‚ÇÉ: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M‚ÇÅ: Type u_9) (M‚ÇÇ: Type u_10) (M‚ÇÉ: Type u_11) (N‚ÇÅ: Type u_12) (N‚ÇÇ: Type u_13) (N‚ÇÉ: Type u_14) (N‚ÇÑ: Type u_15) (Œπ: Type u_16) (M‚ÇÑ: Type u_17) (inst‚úù¬π‚Å∑: Semiring R) (inst‚úù¬π‚Å∂: Semiring S) (inst‚úù¬π‚Åµ: Semiring R‚ÇÅ) (inst‚úù¬π‚Å¥: Semiring R‚ÇÇ) (inst‚úù¬π¬≥: Semiring R‚ÇÉ) (inst‚úù¬π¬≤: AddCommMonoid M) (inst‚úù¬π¬π: AddCommMonoid M‚ÇÅ) (inst‚úù¬π‚Å∞: AddCommMonoid M‚ÇÇ) (inst‚úù‚Åπ: AddComm\n",
      "Generated: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R‚ÇÅ: Type u_2) (R‚ÇÇ: Type u_3) (R‚ÇÉ: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M‚ÇÅ: Type u_9) (M‚ÇÇ: Type u_10) (M‚ÇÉ: Type u_11) (N‚ÇÅ: Type u_12) (N‚ÇÇ: Type u_13) (N‚ÇÉ: Type u_14) (N‚ÇÑ: Type u_15) (Œπ: Type u_16) (M‚ÇÑ: Type u_17) (inst‚úù¬π‚Å∑: Semiring R) (inst‚úù¬π‚Å∂: Semiring S) (inst‚úù¬π‚Åµ: Semiring R‚ÇÅ) (inst‚úù¬π‚Å¥: Semiring R‚ÇÇ) (inst‚úù¬π¬≥: Semiring R‚ÇÉ) (inst‚úù¬π¬≤: AddCommMonoid M) (inst‚úù¬π¬π: AddCommMonoid M‚ÇÅ) (inst‚úù¬π‚Å∞: AddCommMonoid M‚ÇÇ) (inst‚úù‚Åπ: AddCommMonoid M‚ÇÉ) (inst‚úù‚Å∏: AddCommMonoid M‚ÇÑ) (inst‚úù‚Å∑: AddCommMonoid N‚ÇÅ) (inst‚úù‚Å∂: AddCommMonoid N‚ÇÇ) (module_M: Module R M) (module_S_M‚ÇÇ: Module S M‚ÇÇ) (œÉ: R ‚Üí+* S) (œÉ': S ‚Üí+* R) (re‚ÇÅ: RingHomInvPair œÉ œÉ') (re‚ÇÇ: RingHomInvPair œÉ' œÉ) (e e': M ‚âÉ‚Çõ‚Çó[œÉ] M‚ÇÇ) (module_M‚ÇÅ: Module R‚ÇÅ M‚ÇÅ) (module_M‚ÇÇ: Module R‚ÇÇ M‚ÇÇ) (module_M‚ÇÉ: Module R‚ÇÉ M‚ÇÉ) (module_N‚ÇÅ: Module R‚ÇÅ N‚ÇÅ) (module_N‚ÇÇ: Module R‚ÇÅ N‚ÇÇ) (œÉ‚ÇÅ‚ÇÇ: R‚ÇÅ ‚Üí+* R‚ÇÇ) (œÉ‚ÇÇ‚ÇÉ: R‚ÇÇ ‚Üí+* R‚ÇÉ) (œÉ‚ÇÅ‚ÇÉ: R‚ÇÅ ‚Üí+* R‚ÇÉ) (œÉ‚ÇÇ‚ÇÅ: R‚ÇÇ ‚Üí+* R‚ÇÅ) (œÉ‚ÇÉ‚ÇÇ: R‚ÇÉ ‚Üí+* R‚ÇÇ) (œÉ‚ÇÉ‚ÇÅ: R‚ÇÉ ‚Üí+* R‚ÇÅ) (inst‚úù‚Åµ: RingHomCompTriple œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÇ‚ÇÉ œÉ‚ÇÅ‚ÇÉ) (inst‚úù‚Å¥: RingHomCompTriple œÉ‚ÇÉ‚ÇÇ œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÉ‚ÇÅ) (re‚ÇÅ‚ÇÇ: RingHomInvPair œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÇ‚ÇÅ) (re‚ÇÇ‚ÇÉ: RingHomInvPair œÉ‚ÇÇ‚ÇÉ œÉ‚ÇÉ‚ÇÇ) (inst‚úù¬≥: RingHomInvPair œÉ‚ÇÅ‚ÇÉ œÉ‚ÇÉ‚ÇÅ) (re‚ÇÇ‚ÇÅ: RingHomInvPair œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÅ‚ÇÇ) (re‚ÇÉ‚ÇÇ: RingHomInvPair œÉ‚ÇÉ‚ÇÇ œÉ‚ÇÇ‚ÇÉ) (inst‚úù¬≤: RingHomInvPair œÉ‚ÇÉ‚ÇÅ œÉ‚ÇÅ‚ÇÉ) (e‚ÇÇ‚ÇÉ: M‚ÇÇ ‚âÉ‚Çõ‚Çó[œÉ‚ÇÇ‚ÇÉ] M‚ÇÉ) (inst‚úù¬π: RingHomCompTriple œÉ‚ÇÇ‚ÇÅ œÉ‚ÇÅ‚ÇÉ œÉ‚ÇÇ‚ÇÉ) (inst‚úù: RingHomCompTriple œÉ‚ÇÉ‚ÇÅ œÉ‚ÇÅ‚ÇÇ œÉ‚ÇÉ‚ÇÇ) (f: M‚ÇÇ ‚Üí‚Çõ‚Çó[œÉ‚ÇÇ‚ÇÉ] M‚ÇÉ) : f.comp ‚Üëe‚ÇÅ‚ÇÇ.symm.symm = g ‚Üî g = f.comp ‚Üëe‚ÇÅ‚ÇÇ :=   ['M‚ÇÅ ‚Üí‚Çõ‚Çó[œÉ‚ÇÅ‚ÇÇ] M‚ÇÇ']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n"
     ]
    }
   ],
   "source": [
    "def predict_hyp(instruction):\n",
    "    input_ids = tokenizer.encode(instruction, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=50) #max_length=MAX_LENGTH)\n",
    "\n",
    "    # Decode the generated output and the true labels\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\\nCASE\", i)\n",
    "    test_case = tokenized_test[i]\n",
    "    generated_text = predict_hyp(test_case['instruction'])\n",
    "    #input_ids = tokenizer.encode(test_case['instruction'], return_tensors='pt').to('cuda')\n",
    "    #print(input_ids)\n",
    "    print(\"inputs:\", test_case['instruction'])\n",
    "    labels = [x for x in test_case['labels'] if x >= 0]\n",
    "    labels = torch.tensor(labels).to('cuda')\n",
    "    print(\"labels:\", tokenizer.decode(labels, skip_special_tokens=True))\n",
    "    #true_text = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "    # Compare the results\n",
    "    print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d5b603",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_hyp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m instructions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheorem mul_right_inv (G: Type u_1) (inst‚úù : Group G) (a : G) : a * a‚Åª¬π = 1 :=\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheorem fact1 (a: ‚Ñù) (b: ‚Ñù) : a * b * 2 ‚â§ a ^ 2 + b ^ 2 :=\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#\"theorem monotone_f (a: ‚Ñù) (b: ‚Ñù) (f‚úù: ‚Ñù ‚Üí ‚Ñù) (h: ‚àÄ {f : ‚Ñù ‚Üí ‚Ñù}, Monotone f ‚Üí ‚àÄ {a b : ‚Ñù}, f a ‚â§ f b ‚Üí a ‚â§ b) (f: ‚Ñù ‚Üí ‚Ñù := fun x ‚Ü¶ 0) : Monotone f :=\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpredict_hyp\u001b[49m(instructions[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_hyp' is not defined"
     ]
    }
   ],
   "source": [
    "instructions = [\n",
    "    \"theorem mul_right_inv (G: Type u_1) (inst‚úù : Group G) (a : G) : a * a‚Åª¬π = 1 :=\",\n",
    "    \"theorem fact1 (a: ‚Ñù) (b: ‚Ñù) : a * b * 2 ‚â§ a ^ 2 + b ^ 2 :=\",\n",
    "    \"theorem x_pos_neg_1 (x: ‚Ñù) : x = 1 ‚à® x = -1 :=\",\n",
    "    \"theorem Equiv.embeddingFinSucc_fst (m n‚úù n: ‚Ñï) (Œπ: Type u_1) : ‚áë((embeddingFinSucc n Œπ) e).fst = ‚áëe ‚àò Fin.succ :=\"\n",
    "    #\"theorem monotone_f (a: ‚Ñù) (b: ‚Ñù) (f‚úù: ‚Ñù ‚Üí ‚Ñù) (h: ‚àÄ {f : ‚Ñù ‚Üí ‚Ñù}, Monotone f ‚Üí ‚àÄ {a b : ‚Ñù}, f a ‚â§ f b ‚Üí a ‚â§ b) (f: ‚Ñù ‚Üí ‚Ñù := fun x ‚Ü¶ 0) : Monotone f :=\"\n",
    "]\n",
    "\n",
    "predict_hyp(instructions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative is:\n",
      "-4*sin(4*x + 7) + cos(x - 9)\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "# Define the variable and the function\n",
    "x = Symbol('x')\n",
    "f = cos(4*x + 7) - sin(9 - x)\n",
    "\n",
    "# Differentiate the function\n",
    "df = diff(f, x)\n",
    "\n",
    "# Simplify the result\n",
    "df_simplified = simplify(df)\n",
    "\n",
    "print(\"The derivative is:\")\n",
    "print(df_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd83738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions:\n",
      "-2685/49 + 22*sqrt(13442)/49\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, sqrt, solve, simplify\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the equation\n",
    "equation = sqrt(8 - 15*x) + sqrt(-8*x - 6) - 11\n",
    "\n",
    "# Solve the equation\n",
    "solutions = solve(equation, x)\n",
    "\n",
    "# Simplify the solutions\n",
    "simplified_solutions = [simplify(sol) for sol in solutions]\n",
    "\n",
    "print(\"Solutions:\")\n",
    "for sol in simplified_solutions:\n",
    "    print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525f56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-125*x**3 + 675*x**2 - 1215*x + 721\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, expand\n",
    "\n",
    "# Define the symbol\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the polynomials\n",
    "p = -8\n",
    "q = -(5*x - 9)**3\n",
    "\n",
    "# Expand q(x)\n",
    "q_expanded = expand(q)\n",
    "\n",
    "# Calculate the sum p(x) + q(x)\n",
    "result = p + q_expanded\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86744273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h'(1) = 40\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "x, y, z, t = sp.symbols('x y z t')\n",
    "\n",
    "# Define functions\n",
    "f = x*y - x*sp.sqrt(z)\n",
    "g = 4*t**2\n",
    "\n",
    "# Compose h(t) = f(g(t))\n",
    "h = f.subs([(x, g), (y, g), (z, g)])\n",
    "\n",
    "# Calculate h'(t)\n",
    "dh_dt = sp.diff(h, t)\n",
    "\n",
    "# Calculate h'(1)\n",
    "result = dh_dt.subs(t, 1)\n",
    "\n",
    "print(f\"h'(1) = {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d4d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indefinite integral of tan^(-1)(2x) with respect to x is:\n",
      "x*atan(2*x) - log(4*x**2 + 1)/4\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, atan, integrate, simplify, log\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the integrand\n",
    "integrand = atan(2*x)\n",
    "\n",
    "# Evaluate the indefinite integral\n",
    "result = integrate(integrand, x)\n",
    "\n",
    "# Simplify the result\n",
    "simplified_result = simplify(result)\n",
    "\n",
    "print(\"The indefinite integral of tan^(-1)(2x) with respect to x is:\")\n",
    "print(simplified_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c8ca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0$"
      ],
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "k = sp.symbols('k')\n",
    "\n",
    "f = 3**(3*(k+1)+3) - 26*(k+1) - 27\n",
    "g = 27 * (3**(3*k+3) - 26*k - 27) + 169 * (4*k + 4)\n",
    "\n",
    "sp.simplify(f-g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
