{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b32bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mathlib__Algebra__Ring__Subring__Pointwise.lean.pkl\n",
      "12680 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_folder = '/home/mcwave/code/automath/atp/datasets/provability/mathlib4_states_w_proof/'\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "for file_name in file_names:\n",
    "    if not file_name.endswith(\"pkl\"):\n",
    "        continue\n",
    "    if not 'Algebra' in file_name:\n",
    "        continue\n",
    "    count += 1\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    print(\"Loading\", file_name)\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    fin = open(file_path, 'rb')\n",
    "    while True:\n",
    "        try:\n",
    "            pair = pickle.load(fin)\n",
    "            data.append(pair) #(pair[1][0], pair[1][2][0]))\n",
    "        except:\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782ea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE_PP:\n",
      "case h\n",
      "M : Type u_1\n",
      "R : Type u_2\n",
      "inst✝² : Monoid M\n",
      "inst✝¹ : Ring R\n",
      "inst✝ : MulSemiringAction M R\n",
      "a : M\n",
      "S : Subring R\n",
      "nvar0 : R\n",
      "⊢ nvar0 ∈ map (MulSemiringAction.toRingHom M R a) S ↔ nvar0 ∈ a • S\n",
      "TACTICS:\n",
      "symm\n",
      "simp [eq_comm (a := a)]\n",
      "cases S\n",
      "rw [smul_neg]\n",
      "rw [← eq_f₀']\n",
      "STATE_PP:\n",
      "theorem Subring.pointwise_smul_def (M: Type u_1) (R: Type u_2) (inst✝²: Monoid M) (inst✝¹: Ring R) (inst✝: MulSemiringAction M R) (a: M) (S: Subring R) (nvar0: R) : nvar0 ∈ map (MulSemiringAction.toRingHom M R a) S ↔ nvar0 ∈ a • S :=\n",
      "\n",
      "HYPOTHESES:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from utils.lean_math_utils import *\n",
    "from utils.lean_theorem_utils import *\n",
    "\n",
    "def count_lines(string):\n",
    "    # Split the string into lines\n",
    "    lines = string.splitlines()\n",
    "    # Count the number of lines\n",
    "    return len(lines)\n",
    "\n",
    "def extract_first_case(state_pp):\n",
    "    state_pp = state_pp.strip()\n",
    "    if not state_pp.startswith('case'):\n",
    "        return state_pp\n",
    "    lines = state_pp.split('\\n')\n",
    "    first_case = []\n",
    "    for line in lines[1:]:\n",
    "        if line.strip().startswith('case'):\n",
    "            break\n",
    "        if line.strip() != '':\n",
    "            first_case.append(line)\n",
    "    return '\\n'.join(first_case)\n",
    "\n",
    "\n",
    "# Params:\n",
    "#   hyp: tuple(name, type)\n",
    "#   tactics: list(tactic)\n",
    "def is_hypothesis_useful(hyp, tactics):\n",
    "    for tactic in tactics:\n",
    "        tokens = tokenize_lean_tactic(tactic)\n",
    "        if hyp[0] in tokens:\n",
    "            idx = tokens.index(hyp[0])\n",
    "            if idx > 0:\n",
    "                if hyp[0].startswith('h'):\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'exact':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'at':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == '[':\n",
    "                    return True\n",
    "                if idx < len(tokens) - 1 and tokens[idx + 1] == ']':\n",
    "                    return True\n",
    "                for operator in TargetNode.operators:\n",
    "                    if operator in hyp[1]:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def create_hypothesis_predict_data(raw_state_pp, tactics, theorem_name):\n",
    "    is_case = raw_state_pp.strip().startswith('case')\n",
    "    state_pp = extract_first_case(raw_state_pp)\n",
    "    if is_case and count_lines(state_pp) < count_lines(raw_state_pp) - 2:\n",
    "        tactics = tactics[0:1]\n",
    "    #\n",
    "    premise = Premise()\n",
    "    premise.theorem_name = theorem_name\n",
    "    premise.parse_state(state_pp)\n",
    "    #\n",
    "    useful_hypotheses, useless_hypotheses = [], OrderedDict()\n",
    "    for hyp in premise.hypotheses.items():\n",
    "        useful = is_hypothesis_useful(hyp, tactics)\n",
    "        if useful:\n",
    "            #print(\"YES:\", hyp)\n",
    "            useful_hypotheses.append(hyp)\n",
    "        else:\n",
    "            #print(\"NO :\", hyp)\n",
    "            useless_hypotheses[hyp[0]] = hyp[1]\n",
    "    premise.hypotheses = useless_hypotheses\n",
    "    return premise, useful_hypotheses\n",
    "\n",
    "idx = 120\n",
    "\n",
    "state_pp = data[idx][1][0]\n",
    "tactics = data[idx][1][2]\n",
    "theorem_name = data[idx][0][3]\n",
    "\n",
    "print(\"STATE_PP:\\n\" + state_pp)\n",
    "\n",
    "print(\"TACTICS:\\n\" + \"\\n\".join(tactics))\n",
    "\n",
    "premise, useful_hypotheses = create_hypothesis_predict_data(state_pp, tactics, theorem_name)\n",
    "\n",
    "print(\"STATE_PP:\\n\" + premise.to_theorem_code())\n",
    "print(\"\\nHYPOTHESES:\\n\", useful_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fadb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3396064\n",
      "Test: 24237\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "MIN_LENGTH = 4\n",
    "\n",
    "TEST_MOD = 130\n",
    "\n",
    "train_state_pps = []\n",
    "test_state_pps = []\n",
    "train_target_hyps = []\n",
    "test_target_hyps = []\n",
    "seen_hashes = set()\n",
    "fin = open('/home/mcwave/code/axiomatization/datasets/mathlib4_all_states_w_proof_hyp_pred.pkl', 'rb')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        premise, hypotheses = pickle.load(fin)\n",
    "        state_pp = premise.to_theorem_code()\n",
    "        target_hyp = str([x[1] for x in hypotheses])\n",
    "        hash_value = hash(state_pp + '|' + target_hyp)\n",
    "        if hash_value in seen_hashes:\n",
    "            continue\n",
    "        else:\n",
    "            seen_hashes.add(hash_value)\n",
    "        #data.append((state_pp, target_hyp))\n",
    "        if len(state_pp) < 4 or len(target_hyp) < 4:\n",
    "            continue\n",
    "        if hash(premise.theorem_name) % TEST_MOD == 0:\n",
    "            test_state_pps.append(state_pp)\n",
    "            test_target_hyps.append(target_hyp)\n",
    "        else:\n",
    "            train_state_pps.append(state_pp)\n",
    "            train_target_hyps.append(target_hyp)\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "fin.close()\n",
    "\n",
    "print(\"Train:\", len(train_state_pps))\n",
    "print(\"Test:\", len(test_state_pps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "model_name = \"morph-labs/morph-prover-v0-7b\" #\"internlm/internlm2-math-7b\" #\"ScalableMath/Lean-STaR-plus\"  # 'Saisam/gpt-neo-math-small' #\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Define the separator token\n",
    "sep_token = \"<sep>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "# Check if the separator token already exists in the vocabulary\n",
    "if sep_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([sep_token])\n",
    "if pad_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Set the separator token\n",
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'sep_token': sep_token,\n",
    "    'pad_token': pad_token\n",
    "})\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"datasets/text_for_tokenization/mathlib4_20240617_bpe_tokenizer\")\n",
    "# # Define the tokens\n",
    "# sep_token = \"<sep>\"\n",
    "# pad_token = \"<pad>\"\n",
    "\n",
    "# # Set the sep_token and pad_token\n",
    "# tokenizer.sep_token = sep_token\n",
    "# tokenizer.pad_token = pad_token\n",
    "# #tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} <sep> {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "    \n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == tokenizer.sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "        else:\n",
    "            sep_token_idx = sep_token_idx.item()\n",
    "\n",
    "        attention_mask[0:end_idx] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx+1] = -100\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# test_dataset = Dataset.from_dict({\n",
    "#     'instruction': test_state_pps,\n",
    "#     'response': test_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_test = test_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     'instruction': train_state_pps,\n",
    "#     'response': train_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_train = train_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920ae58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89734dc8df74f6198ce6d17b3e04dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "#tokenized_train.save_to_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "#tokenized_test.save_to_disk('datasets/predict_hyp_tokenized_test.dataset')\n",
    "\n",
    "tokenized_train = load_from_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "tokenized_test = load_from_disk('datasets/predict_hyp_tokenized_test.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "777956fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'theorem Finset.diffs_union_right (F: Type u_1) (α: Type u_2) (β: Type u_3) (inst✝²: DecidableEq α) (inst✝¹: DecidableEq β) (inst✝: GeneralizedBooleanAlgebra α) (s₁ s₂ t t₁ t₂ u v: Finset α) (a b c: α) (val✝: Multiset α) (nodup✝: val✝.Nodup) (⊢: = val✝, nodup := nodup✝ } \\\\\\\\ (t₁ ∪ t₂ \\\\ t₁) =) ({: = val✝, nodup := nodup✝ } \\\\\\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\\\\\ t₂) :  :=',\n",
       " 'response': \"['= val✝, nodup := nodup✝ } \\\\\\\\\\\\\\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\\\\\\\\\\\\\ t₂']\",\n",
       " 'input_ids': [1,\n",
       "  16980,\n",
       "  3727,\n",
       "  673,\n",
       "  28723,\n",
       "  28715,\n",
       "  17820,\n",
       "  28730,\n",
       "  14324,\n",
       "  28730,\n",
       "  1246,\n",
       "  325,\n",
       "  28765,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28740,\n",
       "  28731,\n",
       "  325,\n",
       "  28948,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28750,\n",
       "  28731,\n",
       "  325,\n",
       "  29152,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28770,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28941,\n",
       "  28747,\n",
       "  6712,\n",
       "  313,\n",
       "  522,\n",
       "  14564,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  30047,\n",
       "  28747,\n",
       "  6712,\n",
       "  313,\n",
       "  522,\n",
       "  14564,\n",
       "  28705,\n",
       "  29152,\n",
       "  28731,\n",
       "  325,\n",
       "  4138,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  3592,\n",
       "  1332,\n",
       "  9925,\n",
       "  2707,\n",
       "  10343,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  28713,\n",
       "  31552,\n",
       "  268,\n",
       "  30202,\n",
       "  261,\n",
       "  261,\n",
       "  31552,\n",
       "  261,\n",
       "  30202,\n",
       "  332,\n",
       "  363,\n",
       "  28747,\n",
       "  3727,\n",
       "  673,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  28708,\n",
       "  287,\n",
       "  277,\n",
       "  28747,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  1052,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  9713,\n",
       "  278,\n",
       "  299,\n",
       "  28705,\n",
       "  28948,\n",
       "  28731,\n",
       "  325,\n",
       "  26177,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28747,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28723,\n",
       "  28759,\n",
       "  350,\n",
       "  715,\n",
       "  28731,\n",
       "  325,\n",
       "  229,\n",
       "  141,\n",
       "  165,\n",
       "  28747,\n",
       "  327,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  325,\n",
       "  28707,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  261,\n",
       "  30202,\n",
       "  414,\n",
       "  261,\n",
       "  31552,\n",
       "  28731,\n",
       "  327,\n",
       "  28731,\n",
       "  13734,\n",
       "  28747,\n",
       "  327,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  2987,\n",
       "  261,\n",
       "  30202,\n",
       "  28731,\n",
       "  714,\n",
       "  28705,\n",
       "  2137,\n",
       "  28705,\n",
       "  32000,\n",
       "  28705,\n",
       "  5936,\n",
       "  28746,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  30202,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  28705,\n",
       "  5936,\n",
       "  28746,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  31552,\n",
       "  28705,\n",
       "  31916,\n",
       "  371,\n",
       "  1414,\n",
       "  2137,\n",
       "  1414,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  28725,\n",
       "  5460,\n",
       "  715,\n",
       "  2137,\n",
       "  5460,\n",
       "  715,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  443,\n",
       "  414,\n",
       "  2214,\n",
       "  28756,\n",
       "  261,\n",
       "  30202,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68be9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db0e3d3fa37496cb02f2d355628da2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32002, 2048, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "#from huggingface_hub import login\n",
    "\n",
    "#login(token=\"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\")\n",
    "\n",
    "model_name = \"google/gemma-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/axiom/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286047' max='1132024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 286047/1132024 12:18:06 < 226:01:02, 1.04 it/s, Epoch 1.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.167491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.169526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/predict-hyp-gemma-2b\",\n",
    "    evaluation_strategy=\"steps\", #\"epochs\"\n",
    "    learning_rate=3e-7,  # PAY ATTENTION TO LEARNING RATE!\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=20000,\n",
    "    eval_steps=20000,\n",
    "    logging_steps=20000,\n",
    "    save_total_limit=3,\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "cp_path = 'datasets/predict-hyp-gemma-2b/checkpoint-240000'\n",
    "\n",
    "trainer.train(cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f642f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> theorem CategoryTheory.mop_rightUnitor (C: Type u₁) (inst✝¹: Category.{v₁, u₁} C) (inst✝: MonoidalCategory C) (X: C) (⊢ ¬¬¬¬(ρ_ X).mop λ_ { unmop: = X }) :  := <sep>  ['¬¬¬(ρ_ X).mop = λ_ { unmop := X }', '= X }']<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458582ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE 0\n",
      "inputs: theorem LinearMap.exact_map_mkQ_range (R: Type u_1) (M: Type u_2) (N: Type u_3) (P: Type u_4) (inst✝⁶: CommRing R) (inst✝⁵: AddCommGroup M) (inst✝⁴: AddCommGroup N) (inst✝³: AddCommGroup P) (inst✝²: Module R M) (inst✝¹: Module R N) (inst✝: Module R P) (y✝: N) : y✝ ∈ Set.range ⇑f ↔ 0 = (range f).mkQ y✝ :=\n",
      "labels:  ['M →ₗ[R] N']\n",
      "Generated: theorem LinearMap.exact_map_mkQ_range (R: Type u_1) (M: Type u_2) (N: Type u_3) (P: Type u_4) (inst✝⁶: CommRing R) (inst✝⁵: AddCommGroup M) (inst✝⁴: AddCommGroup N) (inst✝³: AddCommGroup P) (inst✝²: Module R M) (inst✝¹: Module R N) (inst✝: Module R P) (y✝: N) : y✝ ∈ Set.range ⇑f ↔ 0 = (range f).mkQ y✝ :=   ['M →ₗ[R] N']']'] 0 = 0 ↔ 0 = 0']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 1\n",
      "inputs: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst✝²: CommRing R) (inst✝¹: DistribSMul S R) (inst✝: IsScalarTower S R R) (a x ⊢ Quotient.map' (fun => • x) ⋯ ((mk {: = toFinsupp✝ }) x) = (mk { toFinsupp := toFinsupp✝ }) (a • x)) (toFinsupp✝: AddMonoidAlgebra R ℕ) :  :=\n",
      "labels:  ['= toFinsupp✝ }) x) = (mk { toFinsupp := toFinsupp✝ }) (a • x)']\n",
      "Generated: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst✝²: CommRing R) (inst✝¹: DistribSMul S R) (inst✝: IsScalarTower S R R) (a x ⊢ Quotient.map' (fun => • x) ⋯ ((mk {: = toFinsupp✝ }) x) = (mk { toFinsupp := toFinsupp✝ }) (a • x)) (toFinsupp✝: AddMonoidAlgebra R ℕ) :  :=   ['= toFinsupp✝ }) x) = (mk { toFinsupp := toFinsupp✝ }) (a • x)']']']']']']']']']']']']']\n",
      "\n",
      "CASE 2\n",
      "inputs: theorem Finset.diffs_union_right (F: Type u_1) (α: Type u_2) (β: Type u_3) (inst✝²: DecidableEq α) (inst✝¹: DecidableEq β) (inst✝: GeneralizedBooleanAlgebra α) (s₁ s₂ t t₁ t₂ u v: Finset α) (a b c: α) (val✝: Multiset α) (nodup✝: val✝.Nodup) (⊢: = val✝, nodup := nodup✝ } \\\\ (t₁ ∪ t₂ \\ t₁) =) ({: = val✝, nodup := nodup✝ } \\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\ t₂) :  :=\n",
      "labels:  ['= val✝, nodup := nodup✝ } \\\\\\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\\\\\ t₂']\n",
      "Generated: theorem Finset.diffs_union_right (F: Type u_1) (α: Type u_2) (β: Type u_3) (inst✝²: DecidableEq α) (inst✝¹: DecidableEq β) (inst✝: GeneralizedBooleanAlgebra α) (s₁ s₂ t t₁ t₂ u v: Finset α) (a b c: α) (val✝: Multiset α) (nodup✝: val✝.Nodup) (⊢: = val✝, nodup := nodup✝ } \\\\ (t₁ ∪ t₂ \\ t₁) =) ({: = val✝, nodup := nodup✝ } \\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\ t₂) :  :=   ['= val✝, nodup := nodup✝ } \\\\\\\\ t₁ ∪ { val := val✝, nodup := nodup✝ } \\\\\\\\ t₂']']\n",
      "\n",
      "CASE 3\n",
      "inputs: theorem RootPairing.coreflection_eq_flip_reflection (ι: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst✝⁴: CommRing R) (inst✝³: AddCommGroup M) (inst✝²: Module R M) (inst✝¹: AddCommGroup N) (inst✝: Module R N) (i j: ι) (f: N) (toPerfectPairing✝: PerfectPairing R M N) (root✝: ι ↪ M) (coroot✝: ι ↪ N) (root_coroot_two✝: ∀ (i : ι), (toPerfectPairing✝.toLin (root✝ i)) (coroot✝ i) = 2) (mapsTo_preReflection_root✝ mapsTo_preReflection_coroot✝: ) ((i: ι), MapsTo (⇑(preReflection (coroot✝ i) (toPerfectPairing✝.toLin (root✝ i)))) (range ⇑coroot✝) (range ⇑coroot✝)) (-(({ ({: = toPerfectPairing✝, root := root✝, coroot := coroot✝, root_coroot_two := root_coroot_two✝,) (mapsTo_preReflection_root: = mapsTo_preReflection_root✝,) (root_coroot_two: = root_coroot_two✝, mapsTo_preReflection_root := mapsTo_preReflection_root✝,) ({: = toPerfectPairing✝, root := root✝, coroot := coroot✝,) : f + :=\n",
      "labels: theorem RootPairing.coreflection_eq_flip_reflection (ι: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst✝⁴: CommRing R) (inst✝³: AddCommGroup M) (inst✝²: Module R M) (inst✝¹: AddCommGroup N) (inst✝: Module R N) (i j: ι) (f: N) (toPerfectPairing✝: PerfectPairing R M N) (root✝: ι ↪ M) (coroot✝: ι ↪ N) (root_coroot_two✝: ∀ (i : ι), (toPerfectPairing✝.toLin (root✝ i)) (coroot✝ i) = 2) (mapsTo_preReflection_root✝ mapsTo_preReflection_coroot✝: ) ((i: ι), MapsTo (⇑(preReflection (coroot✝ i) (toPerfectPairing✝.toLin (root✝ i)))) (range ⇑coroot✝) (range \n",
      "Generated: theorem RootPairing.coreflection_eq_flip_reflection (ι: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst✝⁴: CommRing R) (inst✝³: AddCommGroup M) (inst✝²: Module R M) (inst✝¹: AddCommGroup N) (inst✝: Module R N) (i j: ι) (f: N) (toPerfectPairing✝: PerfectPairing R M N) (root✝: ι ↪ M) (coroot✝: ι ↪ N) (root_coroot_two✝: ∀ (i : ι), (toPerfectPairing✝.toLin (root✝ i)) (coroot✝ i) = 2) (mapsTo_preReflection_root✝ mapsTo_preReflection_coroot✝: ) ((i: ι), MapsTo (⇑(preReflection (coroot✝ i) (toPerfectPairing✝.toLin (root✝ i)))) (range ⇑coroot✝) (range ⇑coroot✝)) (-(({ ({: = toPerfectPairing✝, root := root✝, coroot := coroot✝, root_coroot_two := root_coroot_two✝,) (mapsTo_preReflection_root: = mapsTo_preReflection_root✝,) (root_coroot_two: = root_coroot_two✝, mapsTo_preReflection_root := mapsTo_preReflection_root✝,) ({: = toPerfectPairing✝, root := root✝, coroot := coroot✝,) : f + :=   ['ι), MapsTo (⇑(preReflection (coroot✝ i) (toPerfectPairing✝.toLin (root✝ i)))) (range ⇑cor\n",
      "\n",
      "CASE 4\n",
      "inputs: theorem Finsupp.prod_ne_zero_iff (α: Type u_1) (ι: Type u_2) (γ: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst✝⁶: AddCommMonoid A) (inst✝⁵: AddCommMonoid B) (inst✝⁴: AddCommMonoid C) (t: ι → A → C) (h0: ∀ (i : ι), t i 0 = 0) (h1: ∀ (i : ι) (x y : A), t i (x + y) = t i x + t i y) (s: Finset α) (f✝: α → ι →₀ A) (i: ι) (g✝: ι →₀ A) (k: ι → A → γ → B) (x: γ) (β: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S: Type u_15) (inst✝³: Zero α) (inst✝²: CommMonoidWithZero β) (inst✝¹: Nontrivial β) (inst✝: NoZeroDivisors β) (f: ι →₀ α) (a: α) (g: ι → α → β) (⊢ (∀ (i: ι), f i ≠ 0 → g i (f i) ≠ ?m.111048 * 0) ↔ f.prod g ≠ ?m.111048 * 0) : β :=\n",
      "labels: theorem Finsupp.prod_ne_zero_iff (α: Type u_1) (ι: Type u_2) (γ: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst✝⁶: AddCommMonoid A) (inst✝⁵: AddCommMonoid B) (inst✝⁴: AddCommMonoid C) (t: ι → A → C) (h0: ∀ (i : ι), t i 0 = 0) (h1: ∀ (i : ι) (x y : A), t i (x + y) = t i x + t i y) (s: Finset α) (f✝: α → ι →₀ A) (i: ι) (g✝: ι →₀ A) (k: ι → A → γ → B) (x: γ) (β: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S\n",
      "Generated: theorem Finsupp.prod_ne_zero_iff (α: Type u_1) (ι: Type u_2) (γ: Type u_3) (A: Type u_4) (B: Type u_5) (C: Type u_6) (inst✝⁶: AddCommMonoid A) (inst✝⁵: AddCommMonoid B) (inst✝⁴: AddCommMonoid C) (t: ι → A → C) (h0: ∀ (i : ι), t i 0 = 0) (h1: ∀ (i : ι) (x y : A), t i (x + y) = t i x + t i y) (s: Finset α) (f✝: α → ι →₀ A) (i: ι) (g✝: ι →₀ A) (k: ι → A → γ → B) (x: γ) (β: Type u_7) (M: Type u_8) (M': Type u_9) (N: Type u_10) (P: Type u_11) (G: Type u_12) (R: Type u_14) (S: Type u_15) (inst✝³: Zero α) (inst✝²: CommMonoidWithZero β) (inst✝¹: Nontrivial β) (inst✝: NoZeroDivisors β) (f: ι →₀ α) (a: α) (g: ι → α → β) (⊢ (∀ (i: ι), f i ≠ 0 → g i (f i) ≠ ?m.111048 * 0) ↔ f.prod g ≠ ?m.111048 * 0) : β :=   ['0 = 0']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 5\n",
      "inputs: theorem unitInterval.volume_def (nvar0: Set ↑I) (nvar1: ℝ) : ∃ b, (Measure.comap Subtype.val volume) nvar0 = ↑b ∧ b ≤ ⟨nvar1, nvar2⟩ :=\n",
      "labels:  ['0 ≤ nvar1', 'volume nvar0 = ↑⟨nvar1, nvar2⟩']\n",
      "Generated: theorem unitInterval.volume_def (nvar0: Set ↑I) (nvar1: ℝ) : ∃ b, (Measure.comap Subtype.val volume) nvar0 = ↑b ∧ b ≤ ⟨nvar1, nvar2⟩ :=   ['0 ≤ nvar1', 'volume nvar0 = ↑⟨nvar1, nvar2⟩'] '↑⟨nvar1, nvar2⟩ = ↑⟨nvar1, nvar\n",
      "\n",
      "CASE 6\n",
      "inputs: theorem interior_sInter_subset (X: Type u) (Y: Type v) (ι: Sort w) (α: Type u_1) (β: Type u_2) (x nvar0: X) (s s₁ s₂ t: Set X) (p p₁ p₂: X → Prop) (inst✝: TopologicalSpace X) (S: Set (Set X)) : nvar0 ∈ ⋂ s ∈ S, ⋃₀ {t | IsOpen t ∧ t ⊆ s} :=\n",
      "labels:  ['nvar0 ∈ ⋃₀ {t | IsOpen t ∧ t ⊆ ⋂₀ S}']\n",
      "Generated: theorem interior_sInter_subset (X: Type u) (Y: Type v) (ι: Sort w) (α: Type u_1) (β: Type u_2) (x nvar0: X) (s s₁ s₂ t: Set X) (p p₁ p₂: X → Prop) (inst✝: TopologicalSpace X) (S: Set (Set X)) : nvar0 ∈ ⋂ s ∈ S, ⋃₀ {t | IsOpen t ∧ t ⊆ s} :=   ['nvar0 ∈ ⋃₀ {t | IsOpen t ∧ t ⊆ ⋂₀ S}']']']']']']']']']']\n",
      "\n",
      "CASE 7\n",
      "inputs: theorem RootPairing.coreflection_eq_flip_reflection (ι: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst: CommRing R) (inst_1: AddCommGroup M) (inst_3: AddCommGroup N) (inst_4: Module R N) (i j: ι) (f: N) (root: ι ↪ M) (coroot: ι ↪ N) (root_coroot_two: ∀ (i : ι), (toPerfectPairing.toLin (root i)) (coroot i) = 2) (mapsTo_preReflection_root: = mapsTo_preReflection_root,) ((i: ι), MapsTo (⇑(preReflection (coroot i) (toPerfectPairing.toLin (root i)))) (range ⇑coroot) (range ⇑coroot)) (⊢ ({ {: = toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,) :  :=\n",
      "labels:  ['Module R M', '= toPerfectPairing, root := root, coroot := coroot,\n",
      "Generated: theorem RootPairing.coreflection_eq_flip_reflection (ι: Type u_1) (R: Type u_2) (M: Type u_3) (N: Type u_4) (inst: CommRing R) (inst_1: AddCommGroup M) (inst_3: AddCommGroup N) (inst_4: Module R N) (i j: ι) (f: N) (root: ι ↪ M) (coroot: ι ↪ N) (root_coroot_two: ∀ (i : ι), (toPerfectPairing.toLin (root i)) (coroot i) = 2) (mapsTo_preReflection_root: = mapsTo_preReflection_root,) ((i: ι), MapsTo (⇑(preReflection (coroot i) (toPerfectPairing.toLin (root i)))) (range ⇑coroot) (range ⇑coroot)) (⊢ ({ {: = toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,) :  :=   ['Module R M', '= toPerfectPairing, root := root, coroot := coroot, root_coroot_two := root_coroot_two,', 'ι), MapsTo (��\n",
      "\n",
      "CASE 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst✝²: CommRing R) (f: R[X]) (inst✝¹: DistribSMul S R) (inst✝: IsScalarTower S R R) (a ⊢ Quotient.map' (fun => x) ⋯ ((mk f) = (mk (a { toFinsupp: = x.1 })) :  :=\n",
      "labels:  ['= x.1 })', '= x.1 })']\n",
      "Generated: theorem AdjoinRoot.smul_mk (R: Type u) (S: Type v) (K: Type w) (inst✝²: CommRing R) (f: R[X]) (inst✝¹: DistribSMul S R) (inst✝: IsScalarTower S R R) (a ⊢ Quotient.map' (fun => x) ⋯ ((mk f) = (mk (a { toFinsupp: = x.1 })) :  :=   ['= x.1 })']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n",
      "\n",
      "CASE 9\n",
      "inputs: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R₁: Type u_2) (R₂: Type u_3) (R₃: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M₁: Type u_9) (M₂: Type u_10) (M₃: Type u_11) (N₁: Type u_12) (N₂: Type u_13) (N₃: Type u_14) (N₄: Type u_15) (ι: Type u_16) (M₄: Type u_17) (inst✝¹⁷: Semiring R) (inst✝¹⁶: Semiring S) (inst✝¹⁵: Semiring R₁) (inst✝¹⁴: Semiring R₂) (inst✝¹³: Semiring R₃) (inst✝¹²: AddCommMonoid M) (inst✝¹¹: AddCommMonoid M₁) (inst✝¹⁰: AddCommMonoid M₂) (inst✝⁹: AddCommMonoid M₃) (inst✝⁸: AddCommMonoid M₄) (inst✝⁷: AddCommMonoid N₁) (inst✝⁶: AddCommMonoid N₂) (module_M: Module R M) (module_S_M₂: Module S M₂) (σ: R →+* S) (σ': S →+* R) (re₁: RingHomInvPair σ σ') (re₂: RingHomInvPair σ' σ) (e e': M ≃ₛₗ[σ] M₂) (module_M₁: Module R₁ M₁) (module_M₂: Module R₂ M₂) (module_M₃: Module R₃ M₃) (module_N₁: Module R₁ N₁) (module_N₂: Module R₁ N₂) (σ₁₂: R₁ →+* R₂) (σ₂₃: R₂ →+* R₃) (σ₁₃: R₁ →+* R₃) (σ₂₁: R₂ →+* R₁) (σ₃₂: R₃ →+* R₂) (σ₃₁: R₃ →+* R₁) (inst✝⁵: RingHomCompTriple σ₁₂ σ₂₃ σ₁₃) (inst✝⁴: RingHomCompTriple σ₃₂ σ₂₁ σ₃₁) (re₁₂: RingHomInvPair σ₁₂ σ₂₁) (re₂₃: RingHomInvPair σ₂₃ σ₃₂) (inst✝³: RingHomInvPair σ₁₃ σ₃₁) (re₂₁: RingHomInvPair σ₂₁ σ₁₂) (re₃₂: RingHomInvPair σ₃₂ σ₂₃) (inst✝²: RingHomInvPair σ₃₁ σ₁₃) (e₂₃: M₂ ≃ₛₗ[σ₂₃] M₃) (inst✝¹: RingHomCompTriple σ₂₁ σ₁₃ σ₂₃) (inst✝: RingHomCompTriple σ₃₁ σ₁₂ σ₃₂) (f: M₂ →ₛₗ[σ₂₃] M₃) : f.comp ↑e₁₂.symm.symm = g ↔ g = f.comp ↑e₁₂ :=\n",
      "labels: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R₁: Type u_2) (R₂: Type u_3) (R₃: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M₁: Type u_9) (M₂: Type u_10) (M₃: Type u_11) (N₁: Type u_12) (N₂: Type u_13) (N₃: Type u_14) (N₄: Type u_15) (ι: Type u_16) (M₄: Type u_17) (inst✝¹⁷: Semiring R) (inst✝¹⁶: Semiring S) (inst✝¹⁵: Semiring R₁) (inst✝¹⁴: Semiring R₂) (inst✝¹³: Semiring R₃) (inst✝¹²: AddCommMonoid M) (inst✝¹¹: AddCommMonoid M₁) (inst✝¹⁰: AddCommMonoid M₂) (inst✝⁹: AddComm\n",
      "Generated: theorem LinearEquiv.comp_toLinearMap_symm_eq (R: Type u_1) (R₁: Type u_2) (R₂: Type u_3) (R₃: Type u_4) (k: Type u_5) (K: Type u_6) (S: Type u_7) (M: Type u_8) (M₁: Type u_9) (M₂: Type u_10) (M₃: Type u_11) (N₁: Type u_12) (N₂: Type u_13) (N₃: Type u_14) (N₄: Type u_15) (ι: Type u_16) (M₄: Type u_17) (inst✝¹⁷: Semiring R) (inst✝¹⁶: Semiring S) (inst✝¹⁵: Semiring R₁) (inst✝¹⁴: Semiring R₂) (inst✝¹³: Semiring R₃) (inst✝¹²: AddCommMonoid M) (inst✝¹¹: AddCommMonoid M₁) (inst✝¹⁰: AddCommMonoid M₂) (inst✝⁹: AddCommMonoid M₃) (inst✝⁸: AddCommMonoid M₄) (inst✝⁷: AddCommMonoid N₁) (inst✝⁶: AddCommMonoid N₂) (module_M: Module R M) (module_S_M₂: Module S M₂) (σ: R →+* S) (σ': S →+* R) (re₁: RingHomInvPair σ σ') (re₂: RingHomInvPair σ' σ) (e e': M ≃ₛₗ[σ] M₂) (module_M₁: Module R₁ M₁) (module_M₂: Module R₂ M₂) (module_M₃: Module R₃ M₃) (module_N₁: Module R₁ N₁) (module_N₂: Module R₁ N₂) (σ₁₂: R₁ →+* R₂) (σ₂₃: R₂ →+* R₃) (σ₁₃: R₁ →+* R₃) (σ₂₁: R₂ →+* R₁) (σ₃₂: R₃ →+* R₂) (σ₃₁: R₃ →+* R₁) (inst✝⁵: RingHomCompTriple σ₁₂ σ₂₃ σ₁₃) (inst✝⁴: RingHomCompTriple σ₃₂ σ₂₁ σ₃₁) (re₁₂: RingHomInvPair σ₁₂ σ₂₁) (re₂₃: RingHomInvPair σ₂₃ σ₃₂) (inst✝³: RingHomInvPair σ₁₃ σ₃₁) (re₂₁: RingHomInvPair σ₂₁ σ₁₂) (re₃₂: RingHomInvPair σ₃₂ σ₂₃) (inst✝²: RingHomInvPair σ₃₁ σ₁₃) (e₂₃: M₂ ≃ₛₗ[σ₂₃] M₃) (inst✝¹: RingHomCompTriple σ₂₁ σ₁₃ σ₂₃) (inst✝: RingHomCompTriple σ₃₁ σ₁₂ σ₃₂) (f: M₂ →ₛₗ[σ₂₃] M₃) : f.comp ↑e₁₂.symm.symm = g ↔ g = f.comp ↑e₁₂ :=   ['M₁ →ₛₗ[σ₁₂] M₂']']']']']']']']']']']']']']']']']']']']']']']']']']']']']']\n"
     ]
    }
   ],
   "source": [
    "def predict_hyp(instruction):\n",
    "    input_ids = tokenizer.encode(instruction, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=50) #max_length=MAX_LENGTH)\n",
    "\n",
    "    # Decode the generated output and the true labels\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\\nCASE\", i)\n",
    "    test_case = tokenized_test[i]\n",
    "    generated_text = predict_hyp(test_case['instruction'])\n",
    "    #input_ids = tokenizer.encode(test_case['instruction'], return_tensors='pt').to('cuda')\n",
    "    #print(input_ids)\n",
    "    print(\"inputs:\", test_case['instruction'])\n",
    "    labels = [x for x in test_case['labels'] if x >= 0]\n",
    "    labels = torch.tensor(labels).to('cuda')\n",
    "    print(\"labels:\", tokenizer.decode(labels, skip_special_tokens=True))\n",
    "    #true_text = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "    # Compare the results\n",
    "    print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d5b603",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_hyp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m instructions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheorem mul_right_inv (G: Type u_1) (inst✝ : Group G) (a : G) : a * a⁻¹ = 1 :=\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheorem fact1 (a: ℝ) (b: ℝ) : a * b * 2 ≤ a ^ 2 + b ^ 2 :=\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#\"theorem monotone_f (a: ℝ) (b: ℝ) (f✝: ℝ → ℝ) (h: ∀ {f : ℝ → ℝ}, Monotone f → ∀ {a b : ℝ}, f a ≤ f b → a ≤ b) (f: ℝ → ℝ := fun x ↦ 0) : Monotone f :=\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpredict_hyp\u001b[49m(instructions[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_hyp' is not defined"
     ]
    }
   ],
   "source": [
    "instructions = [\n",
    "    \"theorem mul_right_inv (G: Type u_1) (inst✝ : Group G) (a : G) : a * a⁻¹ = 1 :=\",\n",
    "    \"theorem fact1 (a: ℝ) (b: ℝ) : a * b * 2 ≤ a ^ 2 + b ^ 2 :=\",\n",
    "    \"theorem x_pos_neg_1 (x: ℝ) : x = 1 ∨ x = -1 :=\",\n",
    "    \"theorem Equiv.embeddingFinSucc_fst (m n✝ n: ℕ) (ι: Type u_1) : ⇑((embeddingFinSucc n ι) e).fst = ⇑e ∘ Fin.succ :=\"\n",
    "    #\"theorem monotone_f (a: ℝ) (b: ℝ) (f✝: ℝ → ℝ) (h: ∀ {f : ℝ → ℝ}, Monotone f → ∀ {a b : ℝ}, f a ≤ f b → a ≤ b) (f: ℝ → ℝ := fun x ↦ 0) : Monotone f :=\"\n",
    "]\n",
    "\n",
    "predict_hyp(instructions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative is:\n",
      "-4*sin(4*x + 7) + cos(x - 9)\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "# Define the variable and the function\n",
    "x = Symbol('x')\n",
    "f = cos(4*x + 7) - sin(9 - x)\n",
    "\n",
    "# Differentiate the function\n",
    "df = diff(f, x)\n",
    "\n",
    "# Simplify the result\n",
    "df_simplified = simplify(df)\n",
    "\n",
    "print(\"The derivative is:\")\n",
    "print(df_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd83738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions:\n",
      "-2685/49 + 22*sqrt(13442)/49\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, sqrt, solve, simplify\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the equation\n",
    "equation = sqrt(8 - 15*x) + sqrt(-8*x - 6) - 11\n",
    "\n",
    "# Solve the equation\n",
    "solutions = solve(equation, x)\n",
    "\n",
    "# Simplify the solutions\n",
    "simplified_solutions = [simplify(sol) for sol in solutions]\n",
    "\n",
    "print(\"Solutions:\")\n",
    "for sol in simplified_solutions:\n",
    "    print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525f56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-125*x**3 + 675*x**2 - 1215*x + 721\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, expand\n",
    "\n",
    "# Define the symbol\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the polynomials\n",
    "p = -8\n",
    "q = -(5*x - 9)**3\n",
    "\n",
    "# Expand q(x)\n",
    "q_expanded = expand(q)\n",
    "\n",
    "# Calculate the sum p(x) + q(x)\n",
    "result = p + q_expanded\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86744273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h'(1) = 40\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "x, y, z, t = sp.symbols('x y z t')\n",
    "\n",
    "# Define functions\n",
    "f = x*y - x*sp.sqrt(z)\n",
    "g = 4*t**2\n",
    "\n",
    "# Compose h(t) = f(g(t))\n",
    "h = f.subs([(x, g), (y, g), (z, g)])\n",
    "\n",
    "# Calculate h'(t)\n",
    "dh_dt = sp.diff(h, t)\n",
    "\n",
    "# Calculate h'(1)\n",
    "result = dh_dt.subs(t, 1)\n",
    "\n",
    "print(f\"h'(1) = {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d4d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indefinite integral of tan^(-1)(2x) with respect to x is:\n",
      "x*atan(2*x) - log(4*x**2 + 1)/4\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, atan, integrate, simplify, log\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the integrand\n",
    "integrand = atan(2*x)\n",
    "\n",
    "# Evaluate the indefinite integral\n",
    "result = integrate(integrand, x)\n",
    "\n",
    "# Simplify the result\n",
    "simplified_result = simplify(result)\n",
    "\n",
    "print(\"The indefinite integral of tan^(-1)(2x) with respect to x is:\")\n",
    "print(simplified_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c8ca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0$"
      ],
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "k = sp.symbols('k')\n",
    "\n",
    "f = 3**(3*(k+1)+3) - 26*(k+1) - 27\n",
    "g = 27 * (3**(3*k+3) - 26*k - 27) + 169 * (4*k + 4)\n",
    "\n",
    "sp.simplify(f-g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
