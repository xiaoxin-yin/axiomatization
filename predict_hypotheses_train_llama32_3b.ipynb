{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b32bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mathlib__Algebra__Ring__Subring__Pointwise.lean.pkl\n",
      "12680 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Disable MPI initialization in DeepSpeed\n",
    "os.environ['DEEPSPEED_FORCE_NO_MPI'] = '1'\n",
    "os.environ['DEEPSPEED_COMM'] = 'nccl'  # Force DeepSpeed to use NCCL backend\n",
    "\n",
    "data_folder = '/home/mcwave/code/automath/atp/datasets/provability/mathlib4_states_w_proof/'\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "for file_name in file_names:\n",
    "    if not file_name.endswith(\"pkl\"):\n",
    "        continue\n",
    "    if not 'Algebra' in file_name:\n",
    "        continue\n",
    "    count += 1\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    print(\"Loading\", file_name)\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    fin = open(file_path, 'rb')\n",
    "    while True:\n",
    "        try:\n",
    "            pair = pickle.load(fin)\n",
    "            data.append(pair) #(pair[1][0], pair[1][2][0]))\n",
    "        except:\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ea1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.lean_math_utils import *\n",
    "from utils.lean_theorem_utils import *\n",
    "\n",
    "def count_lines(string):\n",
    "    # Split the string into lines\n",
    "    lines = string.splitlines()\n",
    "    # Count the number of lines\n",
    "    return len(lines)\n",
    "\n",
    "def extract_first_case(state_pp):\n",
    "    state_pp = state_pp.strip()\n",
    "    if not state_pp.startswith('case'):\n",
    "        return state_pp\n",
    "    lines = state_pp.split('\\n')\n",
    "    first_case = []\n",
    "    for line in lines[1:]:\n",
    "        if line.strip().startswith('case'):\n",
    "            break\n",
    "        if line.strip() != '':\n",
    "            first_case.append(line)\n",
    "    return '\\n'.join(first_case)\n",
    "\n",
    "\n",
    "# Params:\n",
    "#   hyp: tuple(name, type)\n",
    "#   tactics: list(tactic)\n",
    "def is_hypothesis_useful(hyp, tactics):\n",
    "    for tactic in tactics:\n",
    "        tokens = tokenize_lean_tactic(tactic)\n",
    "        if hyp[0] in tokens:\n",
    "            idx = tokens.index(hyp[0])\n",
    "            if idx > 0:\n",
    "                if hyp[0].startswith('h'):\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'exact':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'at':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == '[':\n",
    "                    return True\n",
    "                if idx < len(tokens) - 1 and tokens[idx + 1] == ']':\n",
    "                    return True\n",
    "                for operator in TargetNode.operators:\n",
    "                    if operator in hyp[1]:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def create_hypothesis_predict_data(raw_state_pp, tactics, theorem_name):\n",
    "    is_case = raw_state_pp.strip().startswith('case')\n",
    "    state_pp = extract_first_case(raw_state_pp)\n",
    "    if is_case and count_lines(state_pp) < count_lines(raw_state_pp) - 2:\n",
    "        tactics = tactics[0:1]\n",
    "    #\n",
    "    premise = Premise()\n",
    "    premise.theorem_name = theorem_name\n",
    "    premise.parse_state(state_pp)\n",
    "    #\n",
    "    useful_hypotheses, useless_hypotheses = [], OrderedDict()\n",
    "    for hyp in premise.hypotheses.items():\n",
    "        useful = is_hypothesis_useful(hyp, tactics)\n",
    "        if useful:\n",
    "            #print(\"YES:\", hyp)\n",
    "            useful_hypotheses.append(hyp)\n",
    "        else:\n",
    "            #print(\"NO :\", hyp)\n",
    "            useless_hypotheses[hyp[0]] = hyp[1]\n",
    "    premise.hypotheses = useless_hypotheses\n",
    "    return premise, useful_hypotheses\n",
    "\n",
    "idx = 120\n",
    "\n",
    "state_pp = data[idx][1][0]\n",
    "tactics = data[idx][1][2]\n",
    "theorem_name = data[idx][0][3]\n",
    "\n",
    "print(\"STATE_PP:\\n\" + state_pp)\n",
    "\n",
    "print(\"TACTICS:\\n\" + \"\\n\".join(tactics))\n",
    "\n",
    "premise, useful_hypotheses = create_hypothesis_predict_data(state_pp, tactics, theorem_name)\n",
    "\n",
    "print(\"STATE_PP:\\n\" + premise.to_theorem_code())\n",
    "print(\"\\nHYPOTHESES:\\n\", useful_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fadb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# MIN_LENGTH = 4\n",
    "\n",
    "# TEST_MOD = 130\n",
    "\n",
    "# train_state_pps = []\n",
    "# test_state_pps = []\n",
    "# train_target_hyps = []\n",
    "# test_target_hyps = []\n",
    "# seen_hashes = set()\n",
    "# fin = open('/home/mcwave/code/axiomatization/datasets/mathlib4_all_states_w_proof_hyp_pred.pkl', 'rb')\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         premise, hypotheses = pickle.load(fin)\n",
    "#         state_pp = premise.to_theorem_code()\n",
    "#         target_hyp = str([x[1] for x in hypotheses])\n",
    "#         hash_value = hash(state_pp + '|' + target_hyp)\n",
    "#         if hash_value in seen_hashes:\n",
    "#             continue\n",
    "#         else:\n",
    "#             seen_hashes.add(hash_value)\n",
    "#         #data.append((state_pp, target_hyp))\n",
    "#         if len(state_pp) < 4 or len(target_hyp) < 4:\n",
    "#             continue\n",
    "#         if hash(premise.theorem_name) % TEST_MOD == 0:\n",
    "#             test_state_pps.append(state_pp)\n",
    "#             test_target_hyps.append(target_hyp)\n",
    "#         else:\n",
    "#             train_state_pps.append(state_pp)\n",
    "#             train_target_hyps.append(target_hyp)\n",
    "#     except:\n",
    "#         break\n",
    "    \n",
    "# fin.close()\n",
    "\n",
    "# print(\"Train:\", len(train_state_pps))\n",
    "# print(\"Test:\", len(test_state_pps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" #\"morph-labs/morph-prover-v0-7b\" #\"internlm/internlm2-math-7b\" #\"ScalableMath/Lean-STaR-plus\"  # 'Saisam/gpt-neo-math-small' #\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "sep_token_id = tokenizer(\"I # you\")['input_ids'][-2]\n",
    "print(\"sep_token_id=\", sep_token_id)\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} # {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "    \n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "            #print(\"sep_token_id not found\")\n",
    "        else:\n",
    "            #if len(sep_token_idx) > 1:\n",
    "            #    print(\"sep_token_idx:\", sep_token_idx)\n",
    "            sep_token_idx = sep_token_idx[-1].item()\n",
    "            #print(\"sep_token_id found at\", sep_token_idx)\n",
    "\n",
    "        attention_mask[0:end_idx] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx+1] = -100\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# test_dataset = Dataset.from_dict({\n",
    "#     'instruction': test_state_pps,\n",
    "#     'response': test_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_test = test_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )\n",
    "\n",
    "# # Create the Hugging Face dataset\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     'instruction': train_state_pps,\n",
    "#     'response': train_target_hyps\n",
    "# }).shuffle(seed=42)\n",
    "\n",
    "# # Apply the tokenization and preparation function\n",
    "# tokenized_train = train_dataset.map(\n",
    "#     prepare_data,\n",
    "#     batched=True,\n",
    "#     num_proc=4\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1869eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ae58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "#tokenized_train.save_to_disk('datasets/predict_hyp_tokenized_llama32-3b_train.dataset')\n",
    "#tokenized_test.save_to_disk('datasets/predict_hyp_tokenized_llama32-3b_test.dataset')\n",
    "\n",
    "tokenized_train = load_from_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "tokenized_test = load_from_disk('datasets/predict_hyp_tokenized_test.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "# #from huggingface_hub import login\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments with DeepSpeed integration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/predict-hyp-llama32-3b\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=3e-7,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    "    deepspeed=\"deepspeed_config.json\",  # Include the DeepSpeed config file\n",
    "    gradient_checkpointing=True,        # Optional: further reduce memory usage\n",
    ")\n",
    "\n",
    "# Load the model AFTER defining training_args\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Resize token embeddings if necessary\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_test[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458582ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hyp(instruction):\n",
    "    input_ids = tokenizer.encode(instruction, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=50) #max_length=MAX_LENGTH)\n",
    "\n",
    "    # Decode the generated output and the true labels\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\\nCASE\", i)\n",
    "    test_case = tokenized_test[i]\n",
    "    generated_text = predict_hyp(test_case['instruction'])\n",
    "    #input_ids = tokenizer.encode(test_case['instruction'], return_tensors='pt').to('cuda')\n",
    "    #print(input_ids)\n",
    "    print(\"inputs:\", test_case['instruction'])\n",
    "    labels = [x for x in test_case['labels'] if x >= 0]\n",
    "    labels = torch.tensor(labels).to('cuda')\n",
    "    print(\"labels:\", tokenizer.decode(labels, skip_special_tokens=True))\n",
    "    #true_text = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "    # Compare the results\n",
    "    print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"theorem mul_right_inv (G: Type u_1) (inst✝ : Group G) (a : G) : a * a⁻¹ = 1 :=\",\n",
    "    \"theorem fact1 (a: ℝ) (b: ℝ) : a * b * 2 ≤ a ^ 2 + b ^ 2 :=\",\n",
    "    \"theorem x_pos_neg_1 (x: ℝ) : x = 1 ∨ x = -1 :=\",\n",
    "    \"theorem Equiv.embeddingFinSucc_fst (m n✝ n: ℕ) (ι: Type u_1) : ⇑((embeddingFinSucc n ι) e).fst = ⇑e ∘ Fin.succ :=\"\n",
    "    #\"theorem monotone_f (a: ℝ) (b: ℝ) (f✝: ℝ → ℝ) (h: ∀ {f : ℝ → ℝ}, Monotone f → ∀ {a b : ℝ}, f a ≤ f b → a ≤ b) (f: ℝ → ℝ := fun x ↦ 0) : Monotone f :=\"\n",
    "]\n",
    "\n",
    "predict_hyp(instructions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "\n",
    "# Define the variable and the function\n",
    "x = Symbol('x')\n",
    "f = cos(4*x + 7) - sin(9 - x)\n",
    "\n",
    "# Differentiate the function\n",
    "df = diff(f, x)\n",
    "\n",
    "# Simplify the result\n",
    "df_simplified = simplify(df)\n",
    "\n",
    "print(\"The derivative is:\")\n",
    "print(df_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd83738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, sqrt, solve, simplify\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the equation\n",
    "equation = sqrt(8 - 15*x) + sqrt(-8*x - 6) - 11\n",
    "\n",
    "# Solve the equation\n",
    "solutions = solve(equation, x)\n",
    "\n",
    "# Simplify the solutions\n",
    "simplified_solutions = [simplify(sol) for sol in solutions]\n",
    "\n",
    "print(\"Solutions:\")\n",
    "for sol in simplified_solutions:\n",
    "    print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, expand\n",
    "\n",
    "# Define the symbol\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the polynomials\n",
    "p = -8\n",
    "q = -(5*x - 9)**3\n",
    "\n",
    "# Expand q(x)\n",
    "q_expanded = expand(q)\n",
    "\n",
    "# Calculate the sum p(x) + q(x)\n",
    "result = p + q_expanded\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86744273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "x, y, z, t = sp.symbols('x y z t')\n",
    "\n",
    "# Define functions\n",
    "f = x*y - x*sp.sqrt(z)\n",
    "g = 4*t**2\n",
    "\n",
    "# Compose h(t) = f(g(t))\n",
    "h = f.subs([(x, g), (y, g), (z, g)])\n",
    "\n",
    "# Calculate h'(t)\n",
    "dh_dt = sp.diff(h, t)\n",
    "\n",
    "# Calculate h'(1)\n",
    "result = dh_dt.subs(t, 1)\n",
    "\n",
    "print(f\"h'(1) = {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, atan, integrate, simplify, log\n",
    "\n",
    "# Define the variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the integrand\n",
    "integrand = atan(2*x)\n",
    "\n",
    "# Evaluate the indefinite integral\n",
    "result = integrate(integrand, x)\n",
    "\n",
    "# Simplify the result\n",
    "simplified_result = simplify(result)\n",
    "\n",
    "print(\"The indefinite integral of tan^(-1)(2x) with respect to x is:\")\n",
    "print(simplified_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "k = sp.symbols('k')\n",
    "\n",
    "f = 3**(3*(k+1)+3) - 26*(k+1) - 27\n",
    "g = 27 * (3**(3*k+3) - 26*k - 27) + 169 * (4*k + 4)\n",
    "\n",
    "sp.simplify(f-g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
