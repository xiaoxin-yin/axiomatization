{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b32bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mathlib__Algebra__Ring__Subring__Pointwise.lean.pkl\n",
      "12680 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_folder = '/home/mcwave/code/automath/atp/datasets/provability/mathlib4_states_w_proof/'\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "data = []\n",
    "\n",
    "count = 0\n",
    "for file_name in file_names:\n",
    "    if not file_name.endswith(\"pkl\"):\n",
    "        continue\n",
    "    if not 'Algebra' in file_name:\n",
    "        continue\n",
    "    count += 1\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    print(\"Loading\", file_name)\n",
    "    file_path = os.path.join(data_folder, file_name)\n",
    "    fin = open(file_path, 'rb')\n",
    "    while True:\n",
    "        try:\n",
    "            pair = pickle.load(fin)\n",
    "            data.append(pair) #(pair[1][0], pair[1][2][0]))\n",
    "        except:\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782ea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE_PP:\n",
      "case h\n",
      "M : Type u_1\n",
      "R : Type u_2\n",
      "inst✝² : Monoid M\n",
      "inst✝¹ : Ring R\n",
      "inst✝ : MulSemiringAction M R\n",
      "a : M\n",
      "S : Subring R\n",
      "nvar0 : R\n",
      "⊢ nvar0 ∈ map (MulSemiringAction.toRingHom M R a) S ↔ nvar0 ∈ a • S\n",
      "TACTICS:\n",
      "symm\n",
      "simp [eq_comm (a := a)]\n",
      "cases S\n",
      "rw [smul_neg]\n",
      "rw [← eq_f₀']\n",
      "STATE_PP:\n",
      "theorem Subring.pointwise_smul_def (M: Type u_1) (R: Type u_2) (inst✝²: Monoid M) (inst✝¹: Ring R) (inst✝: MulSemiringAction M R) (a: M) (S: Subring R) (nvar0: R) : nvar0 ∈ map (MulSemiringAction.toRingHom M R a) S ↔ nvar0 ∈ a • S :=\n",
      "\n",
      "HYPOTHESES:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from utils.lean_math_utils import *\n",
    "from utils.lean_theorem_utils import *\n",
    "\n",
    "def count_lines(string):\n",
    "    # Split the string into lines\n",
    "    lines = string.splitlines()\n",
    "    # Count the number of lines\n",
    "    return len(lines)\n",
    "\n",
    "def extract_first_case(state_pp):\n",
    "    state_pp = state_pp.strip()\n",
    "    if not state_pp.startswith('case'):\n",
    "        return state_pp\n",
    "    lines = state_pp.split('\\n')\n",
    "    first_case = []\n",
    "    for line in lines[1:]:\n",
    "        if line.strip().startswith('case'):\n",
    "            break\n",
    "        if line.strip() != '':\n",
    "            first_case.append(line)\n",
    "    return '\\n'.join(first_case)\n",
    "\n",
    "\n",
    "# Params:\n",
    "#   hyp: tuple(name, type)\n",
    "#   tactics: list(tactic)\n",
    "def is_hypothesis_useful(hyp, tactics):\n",
    "    for tactic in tactics:\n",
    "        tokens = tokenize_lean_tactic(tactic)\n",
    "        if hyp[0] in tokens:\n",
    "            idx = tokens.index(hyp[0])\n",
    "            if idx > 0:\n",
    "                if hyp[0].startswith('h'):\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'exact':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == 'at':\n",
    "                    return True\n",
    "                if tokens[idx - 1] == '[':\n",
    "                    return True\n",
    "                if idx < len(tokens) - 1 and tokens[idx + 1] == ']':\n",
    "                    return True\n",
    "                for operator in TargetNode.operators:\n",
    "                    if operator in hyp[1]:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def create_hypothesis_predict_data(raw_state_pp, tactics, theorem_name):\n",
    "    is_case = raw_state_pp.strip().startswith('case')\n",
    "    state_pp = extract_first_case(raw_state_pp)\n",
    "    if is_case and count_lines(state_pp) < count_lines(raw_state_pp) - 2:\n",
    "        tactics = tactics[0:1]\n",
    "    #\n",
    "    premise = Premise()\n",
    "    premise.theorem_name = theorem_name\n",
    "    premise.parse_state(state_pp)\n",
    "    #\n",
    "    useful_hypotheses, useless_hypotheses = [], OrderedDict()\n",
    "    for hyp in premise.hypotheses.items():\n",
    "        useful = is_hypothesis_useful(hyp, tactics)\n",
    "        if useful:\n",
    "            #print(\"YES:\", hyp)\n",
    "            useful_hypotheses.append(hyp)\n",
    "        else:\n",
    "            #print(\"NO :\", hyp)\n",
    "            useless_hypotheses[hyp[0]] = hyp[1]\n",
    "    premise.hypotheses = useless_hypotheses\n",
    "    return premise, useful_hypotheses\n",
    "\n",
    "idx = 120\n",
    "\n",
    "state_pp = data[idx][1][0]\n",
    "tactics = data[idx][1][2]\n",
    "theorem_name = data[idx][0][3]\n",
    "\n",
    "print(\"STATE_PP:\\n\" + state_pp)\n",
    "\n",
    "print(\"TACTICS:\\n\" + \"\\n\".join(tactics))\n",
    "\n",
    "premise, useful_hypotheses = create_hypothesis_predict_data(state_pp, tactics, theorem_name)\n",
    "\n",
    "print(\"STATE_PP:\\n\" + premise.to_theorem_code())\n",
    "print(\"\\nHYPOTHESES:\\n\", useful_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fadb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3420301\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "MIN_LENGTH = 4\n",
    "\n",
    "state_pps = []\n",
    "target_hyps = []\n",
    "seen_hashes = set()\n",
    "fin = open('/home/mcwave/code/axiomatization/datasets/mathlib4_all_states_w_proof_hyp_pred.pkl', 'rb')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        premise, hypotheses = pickle.load(fin)\n",
    "        state_pp = premise.to_theorem_code()\n",
    "        target_hyp = str([x[1] for x in hypotheses])\n",
    "        hash_value = hash(state_pp + '|' + target_hyp)\n",
    "        if hash_value in seen_hashes:\n",
    "            continue\n",
    "        else:\n",
    "            seen_hashes.add(hash_value)\n",
    "        #data.append((state_pp, target_hyp))\n",
    "        if len(state_pp) < 4 or len(target_hyp) < 4:\n",
    "            continue\n",
    "        state_pps.append(state_pp)\n",
    "        target_hyps.append(target_hyp)\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "fin.close()\n",
    "\n",
    "print(len(state_pps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb771e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_pps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack(attention_masks),\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack(labels)\n\u001b[1;32m     80\u001b[0m     }\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Assuming you have two lists: instructions and responses\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m instructions \u001b[38;5;241m=\u001b[39m \u001b[43mstate_pps\u001b[49m \u001b[38;5;66;03m#[\"Instruction 1\", \"Instruction 2\"]\u001b[39;00m\n\u001b[1;32m     84\u001b[0m responses \u001b[38;5;241m=\u001b[39m target_hyps \u001b[38;5;66;03m#[\"Response 1\", \"Response 2\"]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#instructions = [\"Instruction 1\", \"Instruction 2\"]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#responses = [\"Response 1\", \"Response 2\"]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Create the Hugging Face dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_pps' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "\n",
    "model_name = \"morph-labs/morph-prover-v0-7b\" #\"internlm/internlm2-math-7b\" #\"ScalableMath/Lean-STaR-plus\"  # 'Saisam/gpt-neo-math-small' #\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Define the separator token\n",
    "sep_token = \"<sep>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "# Check if the separator token already exists in the vocabulary\n",
    "if sep_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([sep_token])\n",
    "if pad_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Set the separator token\n",
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'sep_token': sep_token,\n",
    "    'pad_token': pad_token\n",
    "})\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"datasets/text_for_tokenization/mathlib4_20240617_bpe_tokenizer\")\n",
    "# # Define the tokens\n",
    "# sep_token = \"<sep>\"\n",
    "# pad_token = \"<pad>\"\n",
    "\n",
    "# # Set the sep_token and pad_token\n",
    "# tokenizer.sep_token = sep_token\n",
    "# tokenizer.pad_token = pad_token\n",
    "# #tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} <sep> {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "    \n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == tokenizer.sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "        else:\n",
    "            sep_token_idx = sep_token_idx.item()\n",
    "\n",
    "        attention_mask[0:end_idx] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx+1] = -100\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# Assuming you have two lists: instructions and responses\n",
    "instructions = state_pps #[\"Instruction 1\", \"Instruction 2\"]\n",
    "responses = target_hyps #[\"Response 1\", \"Response 2\"]\n",
    "#instructions = [\"Instruction 1\", \"Instruction 2\"]\n",
    "#responses = [\"Response 1\", \"Response 2\"]\n",
    "\n",
    "\n",
    "# Create the Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'instruction': instructions,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Apply the tokenization and preparation function\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_data,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    "    #remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920ae58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9887d42ff91140ed873cd08522887c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "#tmp = tokenized_dataset.shuffle(seed=42).train_test_split(test_size=0.007, seed=42)\n",
    "#tokenized_train = tmp['train']\n",
    "#tokenized_test = tmp['test']\n",
    "\n",
    "#tokenized_train.save_to_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "#tokenized_test.save_to_disk('datasets/predict_hyp_tokenized_test.dataset')\n",
    "\n",
    "tokenized_train = load_from_disk('datasets/predict_hyp_tokenized_train.dataset')\n",
    "tokenized_test = load_from_disk('datasets/predict_hyp_tokenized_test.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca5b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearMap.exact_map_mkQ_range'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_theorems = [.split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68be9067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbcc2cc39324b6a8fa2bacb20189225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32002, 2304, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "#from huggingface_hub import login\n",
    "\n",
    "#login(token=\"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\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/axiom/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41421' max='970388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 41421/970388 12:34:47 < 282:09:02, 0.91 it/s, Epoch 0.17/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.319200</td>\n",
       "      <td>0.255280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.216810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/predict-hyp-gemma2-2b\",\n",
    "    evaluation_strategy=\"steps\", #\"epochs\"\n",
    "    learning_rate=2e-5,  # PAY ATTENTION TO LEARNING RATE!\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=14,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=20000,\n",
    "    eval_steps=20000,\n",
    "    logging_steps=20000,\n",
    "    save_total_limit=3,\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "cp_path = 'datasets/predict-hyp-gemma2-2b/checkpoint-40000'\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f185fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> theorem CategoryTheory.mop_rightUnitor (C: Type u₁) (inst✝¹: Category.{v₁, u₁} C) (inst✝: MonoidalCategory C) (X: C) (⊢ ¬¬¬¬(ρ_ X).mop λ_ { unmop: = X }) :  := <sep>  ['¬¬¬(ρ_ X).mop = λ_ { unmop := X }', '= X }']<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6536f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'theorem Equiv.embeddingFinSucc_fst (m n✝ n: ℕ) (ι: Type u_1) : ⇑((embeddingFinSucc n ι) e).fst = ⇑e ∘ Fin.succ :=',\n",
       " 'response': \"['Fin (n + 1) ↪ ι']\",\n",
       " 'input_ids': [1,\n",
       "  16980,\n",
       "  8391,\n",
       "  449,\n",
       "  28723,\n",
       "  18320,\n",
       "  3202,\n",
       "  11491,\n",
       "  5173,\n",
       "  588,\n",
       "  28730,\n",
       "  28722,\n",
       "  303,\n",
       "  325,\n",
       "  28719,\n",
       "  307,\n",
       "  229,\n",
       "  159,\n",
       "  160,\n",
       "  307,\n",
       "  28747,\n",
       "  28705,\n",
       "  229,\n",
       "  135,\n",
       "  152,\n",
       "  28731,\n",
       "  325,\n",
       "  28980,\n",
       "  28747,\n",
       "  5707,\n",
       "  332,\n",
       "  28730,\n",
       "  28740,\n",
       "  28731,\n",
       "  714,\n",
       "  28705,\n",
       "  229,\n",
       "  138,\n",
       "  148,\n",
       "  1880,\n",
       "  18320,\n",
       "  3202,\n",
       "  11491,\n",
       "  5173,\n",
       "  588,\n",
       "  307,\n",
       "  28705,\n",
       "  28980,\n",
       "  28731,\n",
       "  317,\n",
       "  609,\n",
       "  28722,\n",
       "  303,\n",
       "  327,\n",
       "  28705,\n",
       "  229,\n",
       "  138,\n",
       "  148,\n",
       "  28706,\n",
       "  28705,\n",
       "  229,\n",
       "  139,\n",
       "  155,\n",
       "  3727,\n",
       "  28723,\n",
       "  2913,\n",
       "  588,\n",
       "  2137,\n",
       "  28705,\n",
       "  32000,\n",
       "  28705,\n",
       "  5936,\n",
       "  11491,\n",
       "  325,\n",
       "  28711,\n",
       "  648,\n",
       "  28705,\n",
       "  28740,\n",
       "  28731,\n",
       "  28705,\n",
       "  229,\n",
       "  137,\n",
       "  173,\n",
       "  28705,\n",
       "  28980,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  28705,\n",
       "  5936,\n",
       "  11491,\n",
       "  325,\n",
       "  28711,\n",
       "  648,\n",
       "  28705,\n",
       "  28740,\n",
       "  28731,\n",
       "  28705,\n",
       "  229,\n",
       "  137,\n",
       "  173,\n",
       "  28705,\n",
       "  28980,\n",
       "  1421,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001,\n",
       "  32001]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "447c27b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 16980,  8391,   449, 28723, 18320,  3202, 11491,  5173,   588,\n",
      "         28730, 28722,   303,   325, 28719,   307,   229,   159,   160,   307,\n",
      "         28747, 28705,   229,   135,   152, 28731,   325, 28980, 28747,  5707,\n",
      "           332, 28730, 28740, 28731,   714, 28705,   229,   138,   148,  1880,\n",
      "         18320,  3202, 11491,  5173,   588,   307, 28705, 28980, 28731,   317,\n",
      "           609, 28722,   303,   327, 28705,   229,   138,   148, 28706, 28705,\n",
      "           229,   139,   155,  3727, 28723,  2913,   588,  2137]],\n",
      "       device='cuda:0')\n",
      "inputs: theorem Equiv.embeddingFinSucc_fst (m n✝ n: ℕ) (ι: Type u_1) : ⇑((embeddingFinSucc n ι) e).fst = ⇑e ∘ Fin.succ :=\n",
      "labels:  ['Fin (n + 1) ↪ ι']\n",
      "tensor([[    1, 16980,  8391,   449, 28723, 18320,  3202, 11491,  5173,   588,\n",
      "         28730, 28722,   303,   325, 28719,   307,   229,   159,   160,   307,\n",
      "         28747, 28705,   229,   135,   152, 28731,   325, 28980, 28747,  5707,\n",
      "           332, 28730, 28740, 28731,   714, 28705,   229,   138,   148,  1880,\n",
      "         18320,  3202, 11491,  5173,   588,   307, 28705, 28980, 28731,   317,\n",
      "           609, 28722,   303,   327, 28705,   229,   138,   148, 28706, 28705,\n",
      "           229,   139,   155,  3727, 28723,  2913,   588,  2137, 28705, 32000,\n",
      "         28705,  5936, 11491,   325, 28711,   648, 28705, 28740, 28731, 28705,\n",
      "           229,   137,   173, 28705, 28980,  1421,   228,   184,   149,   228,\n",
      "           184,   153,  1421,   228,   184,   149,   228,   184,   153,  1421,\n",
      "           228,   184,   149,   228,   184,   153,  1421,   228,   184,   149,\n",
      "           228,   184,   153,  1421,   228,   184,   149,   228]],\n",
      "       device='cuda:0')\n",
      "Generated: theorem Equiv.embeddingFinSucc_fst (m n✝ n: ℕ) (ι: Type u_1) : ⇑((embeddingFinSucc n ι) e).fst = ⇑e ∘ Fin.succ :=   ['Fin (n + 1) ↪ ι']ᵒᵖ']ᵒᵖ']ᵒᵖ']ᵒᵖ']����\n",
      "True:  ['Fin (n + 1) ↪ ι']\n"
     ]
    }
   ],
   "source": [
    "test_case = tokenized_test[7]\n",
    "#input_ids = torch.tensor(test_case['input_ids']).unsqueeze(0).to('cuda')  # Add batch dimension\n",
    "input_ids = tokenizer.encode(test_case['instruction'], return_tensors='pt').to('cuda')\n",
    "print(input_ids)\n",
    "print(\"inputs:\", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "labels = [x for x in test_case['labels'] if x >= 0]\n",
    "labels = torch.tensor(labels).to('cuda')\n",
    "print(\"labels:\", tokenizer.decode(labels, skip_special_tokens=True))\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_new_tokens=50) #max_length=MAX_LENGTH)\n",
    "\n",
    "print(outputs)\n",
    "    \n",
    "# Decode the generated output and the true labels\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "true_text = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Generated:\", generated_text)\n",
    "print(\"True:\", true_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
